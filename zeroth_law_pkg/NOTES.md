# Development Notes & Decisions

**NOTE MAINTENANCE:** When adding new dated sections, always include a precise timestamp generated by running `date --iso-8601=seconds` in the terminal.

---

## Current Workflow: Tool Help Baseline Generation & Verification (2025-04-13T16:01:32+08:00)

**Summary:** This section documents the current, authoritative workflow for generating and verifying configuration definitions for external command-line tools integrated with ZLT. This process ensures ZLT's understanding of a tool's interface (stored in YAML) remains synchronized with the tool's actual help output (captured in TXT/JSON baselines).

**Workflow Steps:**

1.  **Capture (`src/zeroth_law/dev_scripts/capture_txt_tty_output.py`):
    *   Executes the target command (e.g., `ruff --help`) within a pseudo-terminal (PTY) environment.
    *   Crucially, the command's output is piped through `cat` (`sh -c 'command | cat'`) within the PTY.
    *   **Rationale:** This captures the layout intended for a terminal but uses `cat` to reliably strip ANSI escape codes (colors, formatting) before further processing.
    *   Returns the resulting raw bytes and exit code.

2.  **Generate (`src/zeroth_law/dev_scripts/generate_baseline_files.py`):
    *   Takes a command string as input.
    *   Calls the Capture script logic.
    *   Decodes the captured bytes (UTF-8).
    *   **Saves Ground Truth:**
        *   `src/zeroth_law/tools/<tool>/txt/<stem>.txt`: Stores the full decoded string for human readability.
        *   `src/zeroth_law/tools/<tool>/json/<stem>.json`: Stores a structured representation:
            *   `command_used`: List of original command parts.
            *   `generation_timestamp`: ISO 8601 timestamp (UTC).
            *   `overall_crc32_hex`: Hex CRC32 of the entire decoded string.
            *   `lines`: List of objects, each containing:
                *   `line_number`: 1-based index.
                *   `raw_content`: The text of the line (from the `cat`-filtered output).
                *   `crc32_hex`: Hex CRC32 of the `raw_content`, calculated using `lib.crc.calculate_hex_crc32`.
    *   **Idempotent:** Re-running this script for the same command overwrites the `.txt` and `.json` files, updating the timestamp.

3.  **AI Interpretation (YAML):
    *   `src/zeroth_law/tools/<tool>/yaml/<stem>.yaml`: This file is created or updated by the AI (or a human).
    *   It represents ZLT's structured understanding of the tool's interface, based on analysis of the corresponding `.json` file.
    *   Contains keys like `command`, `description`, `help_command`, `help_crc32_baseline_file`, `ignored_help_line_crc32s`, `options`, `positional_args`, `subcommands`.
    *   Crucially, `ignored_help_line_crc32s` and `help_line_crc32` within options/args/subcommands list the hex CRC strings from the `.json` file that define those elements.

4.  **Verify (`tests/test_tool_defs/test_crc_consistency.py`):
    *   This `pytest` test automates the consistency check between the JSON ground truth and the YAML interpretation.
    *   It discovers corresponding `.json` and `.yaml` pairs.
    *   **Checks Performed:**
        *   YAML Existence: Fails if `.yaml` is missing for a `.json`.
        *   YAML Load & Path: Fails if `.yaml` is unparseable or `help_crc32_baseline_file` is incorrect.
        *   JSON Load: Fails if `.json` is unparseable.
        *   **CRC Set Comparison:**
            *   Extracts all unique `crc32_hex` values from JSON lines (`json_crc_set`).
            *   Extracts all unique `crc32_hex` values listed anywhere in the YAML (`yaml_crc_set`).
            *   **If `json_crc_set == yaml_crc_set`:** Pass.
            *   **If `json_crc_set != yaml_crc_set`:** Fail, providing specific error messages:
                *   Lists CRCs missing from YAML (`json_crc_set - yaml_crc_set`).
                *   Lists stale/incorrect CRCs in YAML (`yaml_crc_set - json_crc_set`).
                *   Considers the JSON timestamp (`generation_timestamp`) to guide the user (e.g., if JSON is recent, assume YAML needs fixing; if JSON is old, suggest regenerating baseline first).
    *   **Outcome:** This test acts as the gatekeeper, signalling when AI/human intervention is required to update the YAML interpretation based on changes in the ground truth.

**Superseding Notice:** This section describes the current, implemented workflow. Details regarding specific script names, normalization methods (like `COLUMNS=32767`), or validation logic found in older sections (e.g., "Tool Definition Help Validation Refactoring & Enhancement" dated 2025-04-12) are superseded by this description. The core *rationale* in older sections may still be relevant.

---

## Pre-commit, Ruff Format, and IDE Integration (Recovered Discussion)

**Initial Problem:** The `ruff-format` pre-commit hook was causing commit failures even when it successfully formatted files, because pre-commit halts on *any* file modification by a hook. This created friction, conflicting with Zeroth Law's goal of smooth flow.

**Constraint:** Removing `ruff-format` from pre-commit entirely is not allowed, as it serves as a critical final quality gate for consistent style (Zeroth Law Principle #4).

**Exploration & Reasoning:**

1.  **Comparing `ruff format` and `black`:** The initial thought was whether `black` might handle pre-commit differently. Conclusion: No, the standard `black` hook also causes pre-commit to halt on modification. The issue stems from pre-commit's core design (fail on change), not a specific tool's behavior.
2.  **`ruff check --fix` Behavior:** We previously configured `ruff check --fix` *without* the `--exit-non-zero-on-fix` flag. This allows the *linter* hook to fix many stylistic issues (like trailing whitespace) silently (exit 0) during commit. It only fails the commit if it finds errors it *cannot* fix.
3.  **Why the `ruff check --fix` solution doesn't apply to `ruff-format`:** The `--exit-non-zero-on-fix` flag is specific to linters-with-fixing. Formatters like `ruff-format` don't have an equivalent flag because their primary job *is* modification. Their successful operation inherently triggers pre-commit's halt-on-modification behavior.
4.  **Idea: Benign vs. Significant Formatting:** Could we configure `ruff-format` to only apply "benign" fixes silently? Conclusion: No, `ruff-format` applies all rules comprehensively, and pre-commit doesn't have a mechanism to ignore specific "benign" modifications from a hook.
5.  **Idea: Leverage IDE:** The user suggested shifting the focus from Git hooks to the IDE (Cursor/VS Code).

**Solution: IDE Format-on-Save + Pre-commit Safety Net:**

1.  **Configure Format-on-Save:** Set up the IDE (Cursor/VS Code) with the Ruff extension to automatically run `ruff format` every time a Python file is saved.
    *   Key settings: `editor.formatOnSave: true`, `editor.defaultFormatter: charliermarsh.ruff`.
2.  **Keep `ruff-format` in `pre-commit`:** The `ruff-format` hook remains in `.pre-commit-config.yaml`.
3.  **Workflow Result:**
    *   Developer saves file -> IDE runs `ruff format` -> File is instantly corrected.
    *   Developer runs `git add` -> Already formatted file is staged.
    *   Developer runs `git commit` -> `pre-commit` runs hooks.
    *   `ruff-format` hook runs, sees the file is already correct, makes no changes, exits 0.
    *   Commit proceeds smoothly unless other hooks (like `ruff check --fix` finding unfixable errors, or `mypy`) fail.
    *   The pre-commit hook acts as a crucial *safety net* for cases where Format-on-Save might have been missed, but it's no longer the primary source of formatting *friction*.

**Conclusion:** This approach respects the Zeroth Law constraint of guaranteed formatting (via the pre-commit hook) while significantly improving the developer workflow smoothness by leveraging IDE automation.

## Standardization on Poetry (2025-04-09)

**Decision:** Updated the core framework document (`frameworks/python/ZerothLawAIFramework.py313.md`) to replace `micromamba`/`conda` with `poetry` as the sole mandated environment and dependency manager.

**Rationale:**
*   **Consistency:** Aligns the framework with the project's actual implementation, which already uses `poetry`.
*   **Modern Tooling:** Leverages `poetry`'s integrated dependency resolution, environment management, and build system capabilities.
*   **Simplified Workflow:** Streamlines environment setup and CI compared to the previous `poetry` -> export -> `micromamba` workflow. Removed the need for intermediate `requirements.txt` files and `environment.yml` for the core workflow.

**Impact:**
*   Removed sections and examples related to `micromamba`, `environment.yml`, and `poetry export` from the framework document.
*   Added `poetry`-specific workflow instructions, project structure examples, and CI pipeline examples.
*   Ensures the framework documentation accurately reflects the required development practices for projects adhering to it.

## Git Root vs. Python Project Root Discrepancy (2025-04-09T14:23:46+08:00)

**Discovery:** Troubleshooting `pre-commit` failures (specifically `mypy` not finding `pyproject.toml`) revealed that the Git repository root (`/home/trahloc/code/Misc`) is different from the Python project root where `pyproject.toml` resides (`/home/trahloc/code/Misc/zeroth_law`).

**Problem:** `pre-commit` hooks execute with the Git repository root as their default working directory. Tools run via these hooks (especially using `language: system` or `language: python`) might not automatically find project-specific configuration files (`pyproject.toml`) if they are located in a subdirectory relative to the Git root.

**Solution / Implication:**
*   Hooks needing project configuration must be explicitly told where to find it relative to the Git root.
*   Example: The `mypy` hook was updated to include `args: [--config-file=zeroth_law/pyproject.toml]`.
*   Scripts or tools run by hooks must be aware of this potential discrepancy and handle paths accordingly (e.g., locate `pyproject.toml` by searching upwards or using relative paths carefully).
*   **Zeroth Law Principle:** This highlights the need to avoid assumptions about directory structure and explicitly configure or discover paths required by tooling.

**Future Consideration:** For project *templates* generated by this tool, ensure they function correctly regardless of whether the Git root and project root are the same.

## Mandatory Pre-commit Config Location (2025-04-09T14:33:11+08:00)

**Decision:** As a strict requirement aligned with Zeroth Law principles of co-location and explicit project structure, the `.pre-commit-config.yaml` file **must** reside within the Python project root (the directory containing `pyproject.toml`), not the Git repository root.

**Rationale:**
*   Keeps configuration directly alongside the code it governs.
*   Reduces ambiguity about which configuration applies when invoking `pre-commit` manually or via hooks.
*   Makes the project structure more self-contained.

**Implication:**
*   `pre-commit` commands must now always be invoked using the `--config <project_root>/.pre-commit-config.yaml` flag when run from the Git root (or any directory other than the project root).
*   Hook configurations (`entry`, `args`, `exclude` paths) within the config file must be defined relative to the project root, as that will be the working directory when `pre-commit` uses the specified config file.
*   Build/CI processes must account for this non-standard config location.

## Custom Git Hook for Multi-Project Dispatch (2025-04-09T15:31:15+08:00)

**Requirement:** The automatic `git commit` hook must enforce project-specific `pre-commit` configurations located within project subdirectories (e.g., `zeroth_law/`), even within a multi-project monorepo (like `Misc/`). Standard `pre-commit install` does not support this.

**Decision:** Implement custom Git hook management within the `zeroth_law` tool itself.

**Mechanism:**
1.  **`zeroth_law install-git-hook --git-root <path>` command:**
    *   Generates a custom script implementing the multi-project dispatch logic (see details below).
    *   Installs this script to `<git_root>/.git/hooks/pre-commit`.
    *   Warns the user if a root `.pre-commit-config.yaml` is found, suggesting `restore-git-hooks` if the repo isn't multi-project.
2.  **Generated Hook Script Logic:**
    *   Identifies staged files.
    *   Determines unique project roots containing these files (dirs with `.pre-commit-config.yaml`).
    *   If > 1 project: FAIL (multi-project commit blocked).
    *   If = 1 project: Run `pre-commit run --config <proj>/.pre-commit-config.yaml --files [staged project files]`.
    *   If = 0 projects (root files only): Pass silently (Exit 0).
3.  **`zeroth_law restore-git-hooks --git-root <path>` command:**
    *   Runs `pre-commit install` in the `<git_root>` to overwrite the custom hook with the standard one.
4.  **Configuration Files:**
    *   `Misc/.pre-commit-config.yaml`: Minimal, repo-wide checks (or empty).
    *   `Misc/zeroth_law/.pre-commit-config.yaml`: Project-specific checks (`ruff`, `mypy`, etc.).

**Rationale:** This approach explicitly codifies the required multi-project behavior within the Zeroth Law tooling, providing a robust, albeit custom, solution that aligns with the framework's goals for automated enforcement in complex repository structures.

## Vision: ZLT as Central Orchestrator for ZLF Enforcement (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ)

**Decision:** Solidified the long-term vision for the Zeroth Law Tool (ZLT). ZLT is intended to be the **single, programmatic enforcer** of the Zeroth Law Framework (ZLF) principles. It will achieve this by directly orchestrating the execution of specialized "consultant" tools (`ruff`, `mypy`, `pytest`, `pylint R0801`, fuzzers like `Atheris`, etc.) based on ZLF rules and project-specific configuration (primarily `pyproject.toml`).

**Rationale:**
*   **Programmatic Ground Truth:** Provides a deterministic, tool-based assessment of ZLF compliance, minimizing ambiguity and reliance on varying AI interpretations for core checks.
*   **Unified Interface:** Developers (AI or human) interact primarily with `zlt validate` for comprehensive feedback.
*   **Consistent Enforcement:** Ensures checks are run uniformly according to ZLF across different environments (local, CI).
*   **Simplified Workflow:** Reduces the need to manage and invoke numerous separate tools manually or via complex hook/CI configurations *once ZLT is mature*.

**Implications for ZLT Development:**
*   Requires significant ZLT development to implement robust execution wrappers for each consultant tool.
*   Needs a well-defined configuration schema (`pyproject.toml`) for ZLT to manage tool execution parameters (paths, flags, timeouts, targets).
*   Requires logic within ZLT to interpret the exit codes and output of each consultant tool in the context of ZLF.
*   Demands a unified reporting system to present aggregated compliance results.

**Current State vs. Future:** This remains the *aspirational architecture*. Currently, ZLT performs some checks directly and may aggregate others, but full orchestration requires substantial development. Near-term workflows will still involve direct tool usage and ZLT's current, more limited capabilities. The ZLF document (`ZerothLawAIFramework.py313.md`) has been updated to reflect this *target* state.

## Refined ZLT Vision: Broad Consultation & Evidence-Based Optimization (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ)

**Decision:** Further refined the ZLT vision based on discussions prioritizing maximum violation detection over minimal configuration. ZLT will aim to run a broader set of checks from consultant tools by default, handle aggregation internally, and use a separate `ZLT-dev` process for long-term, evidence-based optimization.

**Strategy Details:**
1.  **Broad Default Checks:** ZLT, when executing consultant tools like `pylint`, will use a configuration that enables a wide range of checks by default. Only rules known to directly conflict with preferred tools (like `ruff` for style) or that are trivially redundant will be explicitly blacklisted initially.
2.  **ZLT Handles Aggregation:** A core ZLT task is to gather all violations reported by the various consultants and then aggregate/de-duplicate them. It needs to normalize similar violation types reported by different tools and present a unified list of unique ZLF violations found.
3.  **`ZLT-dev` for Optimization:** A separate development/testing process (`ZLT-dev`) will be created. Its role is to:
    *   Harvest real-world test cases (starting with ZLT's own tests).
    *   Run unrestricted consultant tools against these tests.
    *   Build and maintain a capability map (likely in an SQLite DB) tracking which tool rules detect which ZLF violations in the known samples, highlighting unique vs. overlapping detections.
    *   Periodically use insights from this map to provide *evidence-based recommendations* for potentially optimizing ZLT's *default* active rule configurations (e.g., disabling a `pylint` rule if the map consistently shows `ruff` provides equal or better detection across all known samples).

**Rationale:**
*   **Prioritizes Detection:** Reduces the risk of "false negatives" (missed violations) compared to a strict initial whitelist approach.
*   **Manages Complexity Internally:** Shifts the burden of handling tool overlap and result correlation onto ZLT, simplifying the experience for the end-user/AI developer.
*   **Evidence-Based Optimization:** Avoids premature optimization or assumption-based configuration. Changes to the default active ruleset are driven by data gathered over time by `ZLT-dev`.
*   **Continuous Evolution:** Creates a mechanism for ZLT's configuration strategy to adapt as consultant tools evolve.

**Impact:**
*   Increases the initial implementation complexity for ZLT (aggregation logic).
*   Requires development of the `ZLT-dev` process and capability mapping database.
*   Leads to a potentially more robust detection mechanism in the near term and an adaptable, evidence-based configuration in the long term.
*   `TODO.md` and `ZerothLawAIFramework.py313.md` have been updated to reflect this strategy.

### 4.14 Input Robustness Verification (via Fuzz Testing)
*(Supports Principle #14)*

*   **Purpose:** To ensure modules handling complex or untrusted data are resilient against unexpected, malformed, or potentially malicious inputs that might not be covered by standard TDD/DDT test cases. Fuzzing acts as a stress test for input processing logic.
*   **Requirement Trigger:** ZLF **requires** fuzz testing for modules that:
    *   Parse complex file formats (e.g., configuration files beyond simple key-value, data serialization formats like custom binary protocols, source code).

## Naming Convention: Project Root vs. Package Directory (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ - AI: Run `date --iso-8601=seconds`)

**Problem:** The common Python convention `project/src/project/` leads to ambiguity and communication issues (especially with AI assistants) when differentiating between the project root directory and the Python package directory.

**Decision:** The ZLF mandates that the project root directory name and the primary Python package directory name (within `src/`) **must not** be identical.

The **recommended and adopted convention** for ZLF projects is:
`project_pkg/src/project/`

*   **Project Root:** `project_pkg` (e.g., `zeroth_law_pkg`)
*   **Package Directory:** `project` (e.g., `zeroth_law`)
*   **Imports:** Remain clean (e.g., `from zeroth_law import cli`)

**Rationale:**
*   Provides clear, unambiguous distinction between the containing project directory (`_pkg` suffix) and the importable Python package.
*   Prioritizes a clean, standard import name (`project`), which is used frequently in code and expected by tooling.
*   The suffixed root name (`project_pkg`) is explicit for configuration and navigation.
*   Avoids platform-specific issues (like case sensitivity) and uses standard characters.

**Impact:**
*   Requires renaming existing project root directories (e.g., `zeroth_law` -> `zeroth_law_pkg`).
*   Tooling configurations (like `pyproject.toml`) need to be aware that the package `project` resides in `src/` within the `project_pkg` root.
*   The ZLF document (`frameworks/python/ZerothLawAIFramework.py313.md`) has been updated to reflect this mandate and recommendation in Section 6.3.

## Pre-Commit Hook Environment Issue for MyPy in Monorepo (2025-04-11T01:06:32+08:00)

**Problem:** When running `mypy` via the project-specific `.pre-commit-config.yaml` (invoked by the custom multi-project dispatcher hook in the Git root), the execution environment set up by `pre-commit` caused persistent failures in `mypy`'s module path resolution for the `src`-layout (`src/zeroth_law/`). This occurred despite the dispatcher script successfully changing the working directory to the project root (`zeroth_law_pkg/`). Manifestations included recurring `"Source file found twice"` errors or failures to find `pyproject.toml` or source directories, depending on the specific hook configuration (`language: python` vs `language: system`, different `entry` commands). This indicates an issue with how the hook execution environment (paths, CWD inheritance, command resolution) interacts with `mypy` in this monorepo setup, rather than a fundamental `mypy` bug or configuration error in `pyproject.toml`.

**Failed Attempts (Illustrating Environment Issues):**
*   Using `language: python` with various `args`: Could not reliably resolve paths relative to the CWD set by the dispatcher, often hitting "Source file found twice" or "Cannot find file" errors.
*   Using `language: system` with `entry: poetry run mypy .`: Failed because the nested `poetry run` appeared to lose the CWD context set by the dispatcher.
*   Using `language: system` with `entry: mypy .` (with or without `--config-file`): Failed because `mypy` appeared to execute relative to the original Git root CWD, not the one set by the dispatcher.
*   Using `language: system` with `entry: sh -c 'cd zeroth_law_pkg && mypy . --config-file ./pyproject.toml'`: *Still* failed with "Source file found twice", indicating the subshell environment was also inheriting problematic path resolution or command execution context.
*   Standard `mypy` config adjustments in `pyproject.toml` (`mypy_path`, `explicit-package-bases`, `ignore_errors`) and environment variables (`MYPYPATH`) were ineffective against the hook execution environment issue.

**Working Solution (Hook Execution Workaround):**
The only configuration found to reliably execute `mypy` correctly *within the hook context* involves forcing a specific execution sequence using `language: system`:

```yaml
# In zeroth_law_pkg/.pre-commit-config.yaml
-   repo: local
    hooks:
      - id: mypy
        name: mypy (local project)
        language: system
        # The initial 'echo' appears necessary to establish a stable/correct
        # execution environment for the subsequent 'sh -c' command within the
        # pre-commit + language: system context on this specific setup.
        # It seems to allow 'sh' and its subsequent commands ('cd', 'mypy')
        # to resolve correctly relative to PATH and the CWD.
        entry: echo "Running mypy in zeroth_law_pkg" && sh -c 'cd zeroth_law_pkg && mypy . --config-file ./pyproject.toml'
        pass_filenames: false
        args: []
```

**Rationale for Workaround:** Adding the `echo` *before* the `sh -c '...'` command seems to influence the hook execution environment provided by `pre-commit` when using `language: system`. This allows the subsequent subshell (`sh -c`) to correctly resolve `sh`, `cd`, and `mypy` relative to the PATH and the CWD set by the dispatcher (`zeroth_law_pkg`). Without the preceding `echo`, the environment appeared inconsistent, potentially misinterpreting command paths or CWD. This workaround specifically addresses the hook execution environment quirk encountered in this setup.

## Stable Branch Vision vs. Development Practicality (2025-04-11T09:57:24+08:00)

**Goal:** The ultimate aim remains a `main` branch where the Zeroth Law Framework (ZLF) is strictly enforced, potentially treating all warnings from consultant tools (`ruff`, `mypy`, `pylint`, etc.) as errors. This state relies on the Zeroth Law Tool (ZLT) being mature enough to orchestrate all checks, aggregate results, and provide unified feedback.

**Current Reality:** ZLT is not yet capable of fulfilling this comprehensive orchestration role. Attempting to enforce the strict `main` standard manually or with current pre-commit hooks alone creates excessive friction and hinders development flow, contradicting ZLF principles.

**Decision & Rationale:** As a practical, temporary measure, a `dev` branch has been created from `main`. All active development will occur on `dev` with relaxed standards to facilitate progress until ZLT matures. The "warnings as errors" philosophy is suspended for non-critical issues during this phase on the `dev` branch. The `main` branch is reserved for the future ZLT-enforced state.

**Current Implementation (`dev` branch):**
*   **`pyproject.toml` Adjustment:** The `[tool.ruff.lint]` configuration has been modified:
    *   `select = ["E", "F"]`: Only critical Pyflakes error categories (E, F) are selected for `ruff check`, significantly reducing the number of issues flagged during pre-commit.
    *   `ignore = [...]`: A comprehensive list of specific warning/style rule codes has been added to `ignore` to further minimize noise from less critical checks.
*   **Goal Preservation:** This relaxation is explicitly temporary and applies to the `dev` branch. The long-term goal of a ZLT-enforced `main` branch remains unchanged. The focus is on unblocking development while ZLT evolves.
*   **Formatting:** Consistent formatting is still enforced by `ruff format` via IDE format-on-save, with the `pre-commit` hook acting as a final safety net.

## Finalized tool_mapping.yaml Structure (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ - AI: Run date --iso-8601=seconds)

**Problem:** Previous attempts to structure `option_mappings` in `tool_mapping.yaml` using nested dictionaries or lists of dictionaries failed due to `PyYAML` parsing errors (`mapping values are not allowed here`), despite the structures appearing valid according to the YAML specification. This blocked the goal of creating a rich mapping for a unified CLI interface.

**Goal:** Define a YAML structure that:
1.  Is reliably parsed by `PyYAML`.
2.  Clearly defines the unified options presented by `zlt` for a given action (the "ZLT Interface").
3.  Maps these unified `zlt` options to the specific command-line arguments required by each underlying "consultant" tool.
4.  Implicitly acts as a capability map, showing which tools support which conceptual `zlt` options.
5.  Is information-rich, allowing definitions of option types, descriptions, etc.

**Solution: Separated `zlt_options` and `maps_options`**

```yaml
# Action Definition (e.g., format)
format:
  description: Description of the ZLT action.

  # 1. Define the ZLT unified interface concepts for this action
  zlt_options:
    # Key: Conceptual option name used in zlt CLI (e.g., --quiet)
    quiet:
      # Value: Mapping defining the ZLT option's properties
      type: flag # Type (flag, value)
      # short: -q # Optional: Short flag name for CLI
      description: Unified description for this ZLT option.
    config:
      type: value
      value_type: path # Optional: Hint for CLI type
      description: Unified description.
    # Special entry for positional paths
    paths:
      type: positional
      # default: ["src"] # Optional list of default paths
      description: Unified description for paths.
    # ... other conceptual ZLT options for this action ...

  # 2. Define tools implementing this action
  tools:
    # Key: Unique name for the tool entry
    ruff_format:
      command: ruff format # Base command for the tool
      # Value: Mapping defining the tool and its specific option translations
      maps_options:
        # Key: The name of the option from 'zlt_options' above (e.g., quiet)
        # Value: The specific argument string the *tool* expects (e.g., -q)
        quiet: -q
        config: --config
        paths: null # Indicates tool uses positional paths defined in zlt_options.paths
        # Note: If 'verbose' existed in zlt_options but isn't listed here,
        #       it means ruff_format doesn't support zlt's verbose concept.
    black:
      command: black
      maps_options:
        quiet: --quiet # Black uses a different flag for the same concept
        config: --config
        paths: null
        # ... potentially other mappings specific to black ...
```

**Rationale:**
*   **Parsability:** This structure uses standard nested mappings (`zlt_options`) and simple key-value mappings (`maps_options`) in ways that `PyYAML` consistently handles, avoiding the previous parsing errors.
*   **Unified Interface:** `zlt_options` clearly defines the public-facing options for each `zlt` action.
*   **Translation Layer:** `maps_options` provides the explicit mapping from the `zlt` concept to the tool's specific argument string.
*   **Capability Map:** The presence or absence of a key in `maps_options` indicates whether a specific tool supports that conceptual `zlt` option.
*   **Richness:** `zlt_options` holds the detailed metadata (`type`, `description`, `value_type`).

**Implementation:** This requires refactoring `cli.py` to build Click options from `zlt_options` and `action_runner.py` to use `maps_options` for translating arguments during execution.

## Mandate: Minimize Mocking in Tests (2025-04-12T11:03:18+08:00)

**Decision:** To ensure tests accurately reflect the behavior of the system and its integrated components, the use of mocking (`unittest.mock`, `pytest-mock`) must be strictly limited.

**Mandate:**
1.  **No Mocking Internal Components:** Tests **must not** mock functions, classes, or modules that are part of the `zeroth_law` codebase itself (e.g., `cli.py`, `config_loader.py`, `action_runner.py`, `analyzer/`). Tests should interact with these components through their public APIs or by invoking the CLI.
2.  **No Mocking Accessible External Tools:** Tests **must not** mock the execution (`subprocess.run`) of external tools that are directly accessible and configurable (e.g., `ruff`, `mypy`, `pytest`). Tests requiring interaction with these tools should set up necessary configurations (e.g., temporary `pyproject.toml`, source files) and invoke the actual tools (e.g., via the appropriate `zlt` action command).
3.  **Permitted Mocking:** Mocking is **only permitted** for:
    *   Simulating truly external systems or dependencies (e.g., network services, databases - not currently applicable to ZLT).
    *   Controlling specific environmental conditions that are difficult or impossible to replicate reliably (e.g., specific filesystem states using `Path` method mocks like `is_dir`, `exists`; specific non-deterministic behavior like random number generation).
    *   Testing specific error handling paths for external processes (e.g., mocking `subprocess.run` *only* to simulate a non-zero exit code or specific error output from an external tool, while the success path uses the real tool).

**Rationale:**
*   **Test Reality:** Ensures tests validate the actual integration and behavior of the codebase, reducing the risk of tests passing while the real application fails.
*   **Refactoring Confidence:** Provides greater confidence when refactoring, as tests are coupled to behavior rather than implementation details hidden by mocks.
*   **Maintainability:** Reduces the fragility associated with complex mock setups that can break easily during refactoring.

**Impact:**
*   Requires reviewing and refactoring existing tests (identified in `test_cli.py`, `test_cli_json_output.py`, `test_cli_option_validation.py`) to remove inappropriate mocks.
*   May increase test setup complexity (e.g., creating temporary file structures and configurations).
*   Leads to more robust and reliable tests that provide higher confidence in the system's correctness.

## ZLF Exception: Using JSON for Machine-Generated Data Artifacts (2025-04-12T15:00:00+08:00)

**Context:** The ZLF generally prefers YAML over JSON due to YAML's support for comments, which aligns with Principle #7 (Self-Documenting Code & Explicit Rationale). This is important for configuration files or other formats intended for human/AI understanding and maintenance.

**Specific Case:** The strategy for verifying tool interfaces involves generating baseline data from command `--help` output. This process uses a script (`src/zeroth_law/dev_scripts/generate_baseline_files.py`) to capture help text, normalize it (via `| cat`), calculate CRC32 hashes (overall and per-line), and store this information.

**Decision:** For this *specific type of data artifact* (machine-generated, machine-read baseline data for automated tests), JSON is deemed an acceptable format, constituting a justified exception to the general preference for YAML.

**Rationale:**
*   **Machine-Centric:** The primary consumer of this file is the automated test suite, not a human or AI maintainer reading it for configuration or logic understanding.
*   **No Embedded Rationale Needed:** The *purpose* and *structure* of this data are defined by the generating script and the consuming test logic, not by comments within the file itself. The rationale for the *approach* is documented elsewhere (e.g., `NOTES.md`, commit history).
*   **Robustness & Simplicity:** JSON offers robust handling of text data (including potential special characters in help output via standard escaping) and simpler, less error-prone parsing compared to YAML for purely data-transfer purposes.

**Conclusion:** While YAML remains the preferred format for human/AI-editable configuration within the ZLF, JSON is permitted for machine-generated data artifacts where embedded comments are unnecessary and JSON's parsing simplicity and robustness are advantageous.

## Tool Definition Help Validation Refactoring & Enhancement (2025-04-12T19:19:13+08:00)

**Note:** The specific implementation details regarding normalization and validation logic described below are **superseded** by the workflow outlined in the "Current Workflow: Tool Help Baseline Generation & Verification" section (dated 2025-04-13). However, the problems identified and the rationale for refactoring remain relevant.

**Problem:** Initial tests validating tool definition YAMLs against command `--help` output using CRC32 hashes were brittle and failed inconsistently.
*   Failures occurred due to differences in terminal width affecting line wrapping in help text (`mypy` was particularly problematic).
*   Significant code duplication existed across test files (`tests/test_tool_defs/`).
*   The initial validation logic was complex and potentially incomplete.

**Solution & Rationale (Historical Context):**
1.  **Refactor to `conftest.py`:** Moved common helper functions (`get_command_output`, etc.), constants, and fixtures (`tool_definition`, `baseline_crc_data`) into `tests/test_tool_defs/conftest.py` to eliminate duplication. Test files now import helpers from `conftest`.
2.  **YAML Updates:** Added the `ignored_help_line_crc32s: []` key to relevant tool definition YAMLs and populated them with initial boilerplate/header CRCs.
3.  **Test Cleanup:** Removed redundant or unreliable older tests.

**Result:** The refactoring improved test structure and maintainability, paving the way for the current, more robust verification approach.

## ZLF Principle Tagging Strategy for Tests (2025-04-12T20:03:10+08:00)

**Goal:** Establish a clear and robust mechanism for associating test cases with the specific Zeroth Law Framework (ZLF) principles they aim to validate or exercise. This association is crucial for the `ZLT-dev` capability mapping process.

**Decisions (Based on AI Discussion):**

1.  **Tagging Mechanism (Decorator vs. Comment):**
    *   **Primary Method:** `@zlf_principle([...])` decorator. Chosen for robustness, AST parsability, validation potential, and future extensibility. A placeholder decorator (no runtime logic initially) will be defined.
    *   **Supported Alternative:** `# ZLF: [...]` structured comments. Allowed for lower friction, legacy cases, or simpler environments.
    *   **Precedence:** If both are present on the same element, the decorator's value is used.

2.  **Tagging Granularity & Inheritance:**
    *   **Module Level:** `# ZLF_MODULE: [...]` comment at the top of the file sets a default for all tests within.
    *   **Class Level:** `@zlf_principle([...])` decorator on a test class sets defaults for its methods.
    *   **Function Level:** `@zlf_principle([...])` decorator on a test function provides the most specific tag.
    *   **Inheritance:** Function > Class > Module. Tags at lower levels override or extend (policy TBD, likely override) tags from higher levels. This minimizes boilerplate while maximizing specificity.

3.  **Handling Multiple Principles:**
    *   **Allowed:** Tests can be tagged with multiple ZLF principles.
    *   **Primary Intent:** The *first* principle listed in the tag array is considered the primary intent the test is designed to validate. Subsequent principles represent collateral coverage.
    *   **Example:** `@zlf_principle(["#12", "#6"])` - Primary focus on Clarity (#12), also covers Complexity (#6).

4.  **Parameterized Tests (`@pytest.mark.parametrize`):**
    *   **Tagging:** The `@zlf_principle` tag is applied to the base test function definition.
    *   **Interpretation:** The tag applies equally to all test instances generated by the parametrization. Per-parameter tagging is deemed overly complex for current needs.

**Rationale:** This combined approach balances the need for a robust, parsable standard (decorators) with flexibility (comments, inheritance) and provides clear conventions for handling multiple principles and parametrization, supporting the goals of `ZLT-dev` capability mapping.

## Adapting TDD/DDT for External/Generated Data & AI Integration (2025-04-13T13:30:44+08:00)

**Problem:** Applying pure TDD/DDT is challenging when dealing with external systems (like tool `--help` output) or when integrating AI analysis, as the "expected" state isn't fully determined a priori.

**Approach (ZLF Adaptation):**
1.  **Step 0 (Characterization/Baseline):** Generate the current, real-world data (e.g., using `generate_baseline_json.py` to capture `tool --help` output). This establishes the "source of truth" for the current external state.
2.  **Step 1 (Define Intent/Structure - YAML):** Define the *semantic structure* and *ZLT's understanding* of the tool/data in a configuration file (e.g., tool definition YAMLs in `src/zeroth_law/tools/`). This declares *what* we want ZLT to know.
3.  **Step 2 (Link Structure to Reality - Initial Mapping):** Manually or semi-automatically connect the YAML structure to the baseline data (e.g., map YAML options to specific line CRCs in the JSON baseline). This synchronizes ZLT's understanding with the initial reality.
4.  **Step 3 (Regression Test - Pytest):** Use automated tests (like the CRC validation tests) to *continuously verify* that ZLT's understanding (YAML) remains synchronized with the characterized reality (JSON baseline). These tests act as regression guards.

**Handling Changes:**
*   When a test fails, it signals desynchronization.
*   Regenerate the baseline data (Step 0).
*   Manually inspect the diff between the old/new baseline and the YAML definition.
*   If the external change was intentional, **update the YAML** (Step 2) to realign ZLT's understanding. The test *should* fail until this is done.
*   If the change was unintentional or breaks ZLT's core assumptions, the test has caught a valid regression.

**AI Integration:**
*   **Future Helper:** AI can assist in Step 2 (mapping YAML to baseline data) by performing semantic analysis of help text.
*   **Future Validator:** AI can act as an additional test layer, semantically comparing YAML definitions against baseline content.
*   **Core ZLT:** The validated YAML definitions become the knowledge base for ZLT's AI-driven orchestration and configuration of tools.

**Conclusion:** TDD/DDT adapts by using characterization tests and baselines. The "test-first" focus applies to the *structure* and *validation logic*, while the external data is captured and used for ongoing regression testing. AI enhances this by adding semantic understanding and validation layers.

## Lessons Learned / Dead Ends (Help Validation)

*   **Normalization via `COLUMNS=32767`:** An early attempt to fix inconsistent line wrapping in `--help` output involved setting the `COLUMNS` environment variable to a very large value before capturing output. While helpful, this wasn't universally effective and could still lead to subtle differences. The current approach of piping through `cat` (`sh -c 'command | cat'`) during capture proved more robust and simpler for removing ANSI codes and ensuring consistent line breaks.
*   **Complex Validation Logic:** The previous `pytest` validation involved multiple steps: checking overall hash, then checking for missing expected CRCs, then calculating set differences to find unexpected CRCs. While functional, this was complex. The current approach simplifies this to a direct comparison of the complete CRC sets (`json_crc_set == yaml_crc_set`), which is easier to understand and implement, providing clearer failure modes (missing vs. stale CRCs).

## Mandate: Standardize on `uv` as the Primary Environment/Package Manager (2025-04-13T18:25:00Z) # AI: Run date --iso-8601=seconds

**Decision:** The Zeroth Law Framework (ZLF) will standardize on **`uv`** as the **primary and recommended** tool for Python environment management and package installation for projects adhering to the ZLF.

**Rationale:**
*   **ZLF Opinionation:** Aligns with the ZLF goal of being opinionated about core tooling to ensure consistency and reduce friction.
*   **Standards Alignment (PEP 621):** `uv` natively uses the standard `[project]` table in `pyproject.toml` for dependency specification, promoting interoperability and adherence to modern Python packaging standards.
*   **Performance:** `uv` offers significant speed advantages over Poetry for dependency resolution and installation.
*   **Modern Tooling:** Represents a current and actively developed tool focused on performance and standards compliance.

**Legacy/Transitional Support for Poetry:**
*   Poetry is now considered **legacy/transitional** within the ZLF context.
*   Projects migrating to ZLF may still use Poetry initially.
*   `uv` provides compatibility by being able to install dependencies from an existing `poetry.lock` file via `uv pip sync --resolver=poetry poetry.lock`. This offers a gradual migration path.
*   New ZLF-native projects **must** use `uv` and define dependencies in the `[project]` table.

**Impact:**
*   The ZLF documentation (`ZerothLawAIFramework.py313.md`) will be updated to reflect `uv` as the standard.
*   `pyproject.toml` examples and requirements will use the `[project]` table for dependencies.
*   Scripts and tests interacting with the environment manager (e.g., `generate_baseline_files.py`, `test_tool_integration.py`) must be updated to use `uv` commands or operate correctly within a `uv`-managed environment.
*   CI/CD workflows must be updated to use `uv` for environment setup and dependency installation.
*   `TODO.md` will be updated with specific migration tasks.

## Refined Tool Interface Workflow & AI Interpretation (v3) (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ) # AI: Run `date --iso-8601=seconds`

**Summary:** This section details the current, streamlined workflow for managing external tool interface definitions, emphasizing the separation between automated ground-truth tracking and AI-driven interpretation. This supersedes previous workflows involving YAML or complex per-line CRC checks.

**Workflow Steps:**

1.  **Tool Discovery & Indexing (`tool_discovery.py` / Test Suite):**
    *   Automated tests examine the environment's executables against whitelists/blacklists.
    *   Whitelisted tools are added/verified in a central manifest: `src/zeroth_law/tools/tool_index.json`.
    *   This index maps tool identifiers (e.g., `ruff_check`) to metadata, critically including the **last known CRC32 hash** of the tool's ground-truth `.txt` help file.

2.  **Ground Truth & Skeleton Generation (`generate_baseline_files.py`):**
    *   Captures `tool --help | cat` output into a `.txt` file (`src/zeroth_law/tools/<tool>/<id>.txt`).
    *   Calculates the CRC32 hash of this `.txt` file.
    *   **Updates `tool_index.json`** with the new CRC for the corresponding tool ID.
    *   Generates a **minimal skeleton** `.json` file (`src/zeroth_law/tools/<tool>/<id>.json`). This skeleton contains basic keys (`command_sequence`, `description`, `options`, etc.) with empty/placeholder values. Crucially, the skeleton **includes `metadata.ground_truth_crc` set to `"0x00000000"`**.

3.  **AI Interpretation (`.json` Population):**
    *   **Me (the AI)** reads the ground-truth `.txt` file.
    *   **Me (the AI)** populates the content of the skeleton `.json` file (filling `description`, `options`, `arguments`, etc.) based on the `.txt` content and the schema in `tools/zlt_schema_guidelines.md`.
    *   **Me (the AI)** must update `metadata.ground_truth_crc` to the correct CRC value from `tool_index.json` during population.
    *   **Consistency Constraint:** When updating an existing `.json`, unchanged options/arguments must retain their names and structure to avoid breaking programmatic dependencies. Changes should only reflect actual updates from the `.txt`.

4.  **Verification (`test_txt_json_consistency.py`):**
    *   **Sole Purpose:** Compares the `metadata.ground_truth_crc` value *inside* the `.json` file against the CRC value stored in `tool_index.json` for the same tool ID.
    *   It **does not** check if the file is a skeleton or populated.
    *   Mismatch triggers a test failure with explicit instructions for **Me (the AI)** to re-read the `.txt`, update the `.json` content, ensure consistency, and align the embedded `metadata.ground_truth_crc` with the `tool_index.json` value.

5.  **Population Check (Separate Tests):**
    *   Separate tests (`test_json_is_populated.py`, `test_json_schema_validation.py`) are responsible for verifying that `.json` files are correctly populated and adhere to the schema. This includes checking for non-empty fields and correct structure.

**Rationale:** This workflow leverages automation for tracking ground truth changes (via CRC in the index) and provides structure (via skeleton JSON with a zeroed CRC), while delegating the complex interpretation task to the AI, with clear validation gates (CRC match, population check, schema check) and update instructions.

**Consumption by ZLT:** This populated and validated `.json` file then serves as the essential machine-readable knowledge base consumed by ZLT's orchestration engine (e.g., `action_runner.py`) to correctly execute the tool and translate ZLT-level configurations into tool-specific command-line arguments.

# Process for Populating Tool Definition JSON Files

1.  **Identify Incorrect/Incomplete JSON Files:**
    *   Run `uv run pytest tests/test_tool_defs/`
    *   Identify failures in:
        *   `test_txt_json_consistency.py::test_json_crc_matches_index`: Indicates the `metadata.ground_truth_crc` in the JSON doesn't match the index (it might still be the initial `0x00000000` or an outdated value).
        *   `test_json_is_populated.py`: Indicates the JSON file lacks sufficient populated content (e.g., empty description, no options listed).
        *   `test_json_schema_validation.py`: Indicates the JSON file structure violates the schema.

2.  **Iterate Through Failing Files:**
    *   For each failing tool ID (e.g., `mytool` or `mytool-subcommand`):
        *   **Locate Files:**
            *   JSON: `src/zeroth_law/tools/mytool/mytool.json` (or corresponding path)
            *   Help Text: `src/zeroth_law/tools/mytool/mytool.txt` (or corresponding path)
            *   Index: `src/zeroth_law/tools/tool_index.json`
        *   **Read Help Text:** Use `read_file` on the `.txt` file.
        *   **Get Index CRC:** Extract the expected `ground_truth_crc` for the tool ID from `tool_index.json`.
        *   **Populate/Correct JSON:**
            *   Use `edit_file` on the `.json` file.
            *   Update `metadata.ground_truth_crc` with the correct CRC value from the index.
            *   Fill/Update `description`, `usage`, `options`, `arguments`, `subcommands` based on the `.txt` help text and schema guidelines (`tools/zlt_schema_guidelines.md`), ensuring consistency for unchanged elements and adding any newly discovered elements from the help text.
        *   **Handle Edit Failures:** If `edit_file` reports "no changes" but `pytest` still fails the file, use `reapply` on the target `.json` file.
        *   **Verify:** Rerun `uv run pytest tests/test_tool_defs/` to confirm the test(s) for the specific tool ID now pass.

3.  **Repeat:** Continue until all tests in `tests/test_tool_defs/` pass.

4.  **Cleanup:**
    *   Address any duplicate/redundant tool definitions (e.g., hyphen vs. underscore variants like `isort-identify-imports` vs `isort_identify-imports`). Remove unnecessary files (`.json`, `.txt`) and corresponding entries from `tool_index.json`.
