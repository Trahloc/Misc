# Development Notes & Decisions

**NOTE MAINTENANCE:** When adding new dated sections, always include a precise timestamp generated by running `date --iso-8601=seconds` in the terminal.

---

## Current Workflow: Tool Help Baseline Generation & Verification (2025-04-13T16:01:32+08:00)

**Summary:** This section documents the current, authoritative workflow for generating and verifying configuration definitions for external command-line tools integrated with ZLT. This process ensures ZLT's understanding of a tool's interface (stored in YAML) remains synchronized with the tool's actual help output (captured in TXT/JSON baselines).

**Workflow Steps:**

1.  **Capture (`src/zeroth_law/dev_scripts/capture_txt_tty_output.py`):
    *   Executes the target command (e.g., `ruff --help`) within a pseudo-terminal (PTY) environment.
    *   Crucially, the command's output is piped through `cat` (`sh -c 'command | cat'`) within the PTY.
    *   **Rationale:** This captures the layout intended for a terminal but uses `cat` to reliably strip ANSI escape codes (colors, formatting) before further processing.
    *   Returns the resulting raw bytes and exit code.

2.  **Generate (Likely `src/zeroth_law/dev_scripts/generate_baseline_cli.py` / Test Logic):**
    *   Takes a command string as input.
    *   Calls the Capture script logic.
    *   Decodes the captured bytes (UTF-8).
    *   **Saves Ground Truth:**
        *   `src/zeroth_law/tools/<tool>/txt/<stem>.txt`: Stores the full decoded string for human readability.
        *   `src/zeroth_law/tools/<tool>/json/<stem>.json`: Stores a structured representation:
            *   `command_used`: List of original command parts.
            *   `generation_timestamp`: ISO 8601 timestamp (UTC).
            *   `overall_crc32_hex`: Hex CRC32 of the entire decoded string.
            *   `lines`: List of objects, each containing:
                *   `line_number`: 1-based index.
                *   `raw_content`: The text of the line (from the `cat`-filtered output).
                *   `crc32_hex`: Hex CRC32 of the `raw_content`, calculated using `lib.crc.calculate_hex_crc32`.
    *   **Idempotent:** Re-running this script for the same command overwrites the `.txt` and `.json` files, updating the timestamp.

3.  **AI Interpretation (YAML):
    *   `src/zeroth_law/tools/<tool>/yaml/<stem>.yaml`: This file is created or updated by the AI (or a human).
    *   It represents ZLT's structured understanding of the tool's interface, based on analysis of the corresponding `.json` file.
    *   Contains keys like `command`, `description`, `help_command`, `help_crc32_baseline_file`, `ignored_help_line_crc32s`, `options`, `positional_args`, `subcommands`.
    *   Crucially, `ignored_help_line_crc32s` and `help_line_crc32` within options/args/subcommands list the hex CRC strings from the `.json` file that define those elements.

4.  **Verify (`tests/test_tool_defs/test_crc_consistency.py`):
    *   This `pytest` test automates the consistency check between the JSON ground truth and the YAML interpretation.
    *   It discovers corresponding `.json` and `.yaml` pairs.
    *   **Checks Performed:**
        *   YAML Existence: Fails if `.yaml` is missing for a `.json`.
        *   YAML Load & Path: Fails if `.yaml` is unparseable or `help_crc32_baseline_file` is incorrect.
        *   JSON Load: Fails if `.json` is unparseable.
        *   **CRC Set Comparison:**
            *   Extracts all unique `crc32_hex` values from JSON lines (`json_crc_set`).
            *   Extracts all unique `crc32_hex` values listed anywhere in the YAML (`yaml_crc_set`).
            *   **If `json_crc_set == yaml_crc_set`:** Pass.
            *   **If `json_crc_set != yaml_crc_set`:** Fail, providing specific error messages:
                *   Lists CRCs missing from YAML (`json_crc_set - yaml_crc_set`).
                *   Lists stale/incorrect CRCs in YAML (`yaml_crc_set - json_crc_set`).
                *   Considers the JSON timestamp (`generation_timestamp`) to guide the user (e.g., if JSON is recent, assume YAML needs fixing; if JSON is old, suggest regenerating baseline first).
    *   **Outcome:** This test acts as the gatekeeper, signalling when AI/human intervention is required to update the YAML interpretation based on changes in the ground truth.

**Superseding Notice:** This section describes the current, implemented workflow. Details regarding specific script names, normalization methods (like `COLUMNS=32767`), or validation logic found in older sections (e.g., "Tool Definition Help Validation Refactoring & Enhancement" dated 2025-04-12) are superseded by this description. The core *rationale* in older sections may still be relevant.

---

## Pre-commit, Ruff Format, and IDE Integration (Recovered Discussion)

**Initial Problem:** The `ruff-format` pre-commit hook was causing commit failures even when it successfully formatted files, because pre-commit halts on *any* file modification by a hook. This created friction, conflicting with Zeroth Law's goal of smooth flow.

**Constraint:** Removing `ruff-format` from pre-commit entirely is not allowed, as it serves as a critical final quality gate for consistent style (Zeroth Law Principle #4).

**Exploration & Reasoning:**

1.  **Comparing `ruff format` and `black`:** The initial thought was whether `black` might handle pre-commit differently. Conclusion: No, the standard `black` hook also causes pre-commit to halt on modification. The issue stems from pre-commit's core design (fail on change), not a specific tool's behavior.
2.  **`ruff check --fix` Behavior:** We previously configured `ruff check --fix` *without* the `--exit-non-zero-on-fix` flag. This allows the *linter* hook to fix many stylistic issues (like trailing whitespace) silently (exit 0) during commit. It only fails the commit if it finds errors it *cannot* fix.
3.  **Why the `ruff check --fix` solution doesn't apply to `ruff-format`:** The `--exit-non-zero-on-fix` flag is specific to linters-with-fixing. Formatters like `ruff-format` don't have an equivalent flag because their primary job *is* modification. Their successful operation inherently triggers pre-commit's halt-on-modification behavior.
4.  **Idea: Benign vs. Significant Formatting:** Could we configure `ruff-format` to only apply "benign" fixes silently? Conclusion: No, `ruff-format` applies all rules comprehensively, and pre-commit doesn't have a mechanism to ignore specific "benign" modifications from a hook.
5.  **Idea: Leverage IDE:** The user suggested shifting the focus from Git hooks to the IDE (Cursor/VS Code).

**Solution: IDE Format-on-Save + Pre-commit Safety Net:**

1.  **Configure Format-on-Save:** Set up the IDE (Cursor/VS Code) with the Ruff extension to automatically run `ruff format` every time a Python file is saved.
    *   Key settings: `editor.formatOnSave: true`, `editor.defaultFormatter: charliermarsh.ruff`.
2.  **Keep `ruff-format` in `pre-commit`:** The `ruff-format` hook remains in `.pre-commit-config.yaml`.
3.  **Workflow Result:**
    *   Developer saves file -> IDE runs `ruff format` -> File is instantly corrected.
    *   Developer runs `git add` -> Already formatted file is staged.
    *   Developer runs `git commit` -> `pre-commit` runs hooks.
    *   `ruff-format` hook runs, sees the file is already correct, makes no changes, exits 0.
    *   Commit proceeds smoothly unless other hooks (like `ruff check --fix` finding unfixable errors, or `mypy`) fail.
    *   The pre-commit hook acts as a crucial *safety net* for cases where Format-on-Save might have been missed, but it's no longer the primary source of formatting *friction*.

**Conclusion:** This approach respects the Zeroth Law constraint of guaranteed formatting (via the pre-commit hook) while significantly improving the developer workflow smoothness by leveraging IDE automation.

## Standardization on Poetry (2025-04-09)

**Decision:** Updated the core framework document (`frameworks/python/ZerothLawAIFramework.py313.md`) to replace `micromamba`/`conda` with `poetry` as the sole mandated environment and dependency manager.

**Rationale:**
*   **Consistency:** Aligns the framework with the project's actual implementation, which already uses `poetry`.
*   **Modern Tooling:** Leverages `poetry`'s integrated dependency resolution, environment management, and build system capabilities.
*   **Simplified Workflow:** Streamlines environment setup and CI compared to the previous `poetry` -> export -> `micromamba` workflow. Removed the need for intermediate `requirements.txt` files and `environment.yml` for the core workflow.

**Impact:**
*   Removed sections and examples related to `micromamba`, `environment.yml`, and `poetry export` from the framework document.
*   Added `poetry`-specific workflow instructions, project structure examples, and CI pipeline examples.
*   Ensures the framework documentation accurately reflects the required development practices for projects adhering to it.

## Git Root vs. Python Project Root Discrepancy (2025-04-09T14:23:46+08:00)

**Discovery:** Troubleshooting `pre-commit` failures (specifically `mypy` not finding `pyproject.toml`) revealed that the Git repository root (`/home/trahloc/code/Misc`) is different from the Python project root where `pyproject.toml` resides (`/home/trahloc/code/Misc/zeroth_law`).

**Problem:** `pre-commit` hooks execute with the Git repository root as their default working directory. Tools run via these hooks (especially using `language: system` or `language: python`) might not automatically find project-specific configuration files (`pyproject.toml`) if they are located in a subdirectory relative to the Git root.

**Solution / Implication:**
*   Hooks needing project configuration must be explicitly told where to find it relative to the Git root.
*   Example: The `mypy` hook was updated to include `args: [--config-file=zeroth_law/pyproject.toml]`.
*   Scripts or tools run by hooks must be aware of this potential discrepancy and handle paths accordingly (e.g., locate `pyproject.toml` by searching upwards or using relative paths carefully).
*   **Zeroth Law Principle:** This highlights the need to avoid assumptions about directory structure and explicitly configure or discover paths required by tooling.

**Future Consideration:** For project *templates* generated by this tool, ensure they function correctly regardless of whether the Git root and project root are the same.

## Mandatory Pre-commit Config Location (2025-04-09T14:33:11+08:00)

**Decision:** As a strict requirement aligned with Zeroth Law principles of co-location and explicit project structure, the `.pre-commit-config.yaml` file **must** reside within the Python project root (the directory containing `pyproject.toml`), not the Git repository root.

**Rationale:**
*   Keeps configuration directly alongside the code it governs.
*   Reduces ambiguity about which configuration applies when invoking `pre-commit` manually or via hooks.
*   Makes the project structure more self-contained.

**Implication:**
*   `pre-commit` commands must now always be invoked using the `--config <project_root>/.pre-commit-config.yaml` flag when run from the Git root (or any directory other than the project root).
*   Hook configurations (`entry`, `args`, `exclude` paths) within the config file must be defined relative to the project root, as that will be the working directory when `pre-commit` uses the specified config file.
*   Build/CI processes must account for this non-standard config location.

## Custom Git Hook for Multi-Project Dispatch (2025-04-09T15:31:15+08:00)

**Requirement:** The automatic `git commit` hook must enforce project-specific `pre-commit` configurations located within project subdirectories (e.g., `zeroth_law/`), even within a multi-project monorepo (like `Misc/`). Standard `pre-commit install` does not support this.

**Decision:** Implement custom Git hook management within the `zeroth_law` tool itself.

**Mechanism:**
1.  **`zeroth_law install-git-hook --git-root <path>` command:**
    *   Generates a custom script implementing the multi-project dispatch logic (see details below).
    *   Installs this script to `<git_root>/.git/hooks/pre-commit`.
    *   Warns the user if a root `.pre-commit-config.yaml` is found, suggesting `restore-git-hooks` if the repo isn't multi-project.
2.  **Generated Hook Script Logic:**
    *   Identifies staged files.
    *   Determines unique project roots containing these files (dirs with `.pre-commit-config.yaml`).
    *   If > 1 project: FAIL (multi-project commit blocked).
    *   If = 1 project: Run `pre-commit run --config <proj>/.pre-commit-config.yaml --files [staged project files]`.
    *   If = 0 projects (root files only): Pass silently (Exit 0).
3.  **`zeroth_law restore-git-hooks --git-root <path>` command:**
    *   Runs `pre-commit install` in the `<git_root>` to overwrite the custom hook with the standard one.
4.  **Configuration Files:**
    *   `Misc/.pre-commit-config.yaml`: Minimal, repo-wide checks (or empty).
    *   `Misc/zeroth_law/.pre-commit-config.yaml`: Project-specific checks (`ruff`, `mypy`, etc.).

**Rationale:** This approach explicitly codifies the required multi-project behavior within the Zeroth Law tooling, providing a robust, albeit custom, solution that aligns with the framework's goals for automated enforcement in complex repository structures.

## Vision: ZLT as Central Orchestrator for ZLF Enforcement (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ)

**Decision:** Solidified the long-term vision for the Zeroth Law Tool (ZLT). ZLT is intended to be the **single, programmatic enforcer** of the Zeroth Law Framework (ZLF) principles. It will achieve this by directly orchestrating the execution of specialized "consultant" tools (`ruff`, `mypy`, `pytest`, `pylint R0801`, fuzzers like `Atheris`, etc.) based on ZLF rules and project-specific configuration (primarily `pyproject.toml`).

**Rationale:**
*   **Programmatic Ground Truth:** Provides a deterministic, tool-based assessment of ZLF compliance, minimizing ambiguity and reliance on varying AI interpretations for core checks.
*   **Unified Interface:** Developers (AI or human) interact primarily with `zlt validate` for comprehensive feedback.
*   **Consistent Enforcement:** Ensures checks are run uniformly according to ZLF across different environments (local, CI).
*   **Simplified Workflow:** Reduces the need to manage and invoke numerous separate tools manually or via complex hook/CI configurations *once ZLT is mature*.

**Implications for ZLT Development:**
*   Requires significant ZLT development to implement robust execution wrappers for each consultant tool.
*   Needs a well-defined configuration schema (`pyproject.toml`) for ZLT to manage tool execution parameters (paths, flags, timeouts, targets).
*   Requires logic within ZLT to interpret the exit codes and output of each consultant tool in the context of ZLF.
*   Demands a unified reporting system to present aggregated compliance results.

**Current State vs. Future:** This remains the *aspirational architecture*. Currently, ZLT performs some checks directly and may aggregate others, but full orchestration requires substantial development. Near-term workflows will still involve direct tool usage and ZLT's current, more limited capabilities. The ZLF document (`ZerothLawAIFramework.py313.md`) has been updated to reflect this *target* state.

## Refined ZLT Vision: Broad Consultation & Evidence-Based Optimization (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ)

**Decision:** Further refined the ZLT vision based on discussions prioritizing maximum violation detection over minimal configuration. ZLT will aim to run a broader set of checks from consultant tools by default, handle aggregation internally, and use a separate `ZLT-dev` process for long-term, evidence-based optimization.

**Strategy Details:**
1.  **Broad Default Checks:** ZLT, when executing consultant tools like `pylint`, will use a configuration that enables a wide range of checks by default. Only rules known to directly conflict with preferred tools (like `ruff` for style) or that are trivially redundant will be explicitly blacklisted initially.
2.  **ZLT Handles Aggregation:** A core ZLT task is to gather all violations reported by the various consultants and then aggregate/de-duplicate them. It needs to normalize similar violation types reported by different tools and present a unified list of unique ZLF violations found.
3.  **`ZLT-dev` for Optimization:** A separate development/testing process (`ZLT-dev`) will be created. Its role is to:
    *   Harvest real-world test cases (starting with ZLT's own tests).
    *   Run unrestricted consultant tools against these tests.
    *   Build and maintain a capability map (likely in an SQLite DB) tracking which tool rules detect which ZLF violations in the known samples, highlighting unique vs. overlapping detections.
    *   Periodically use insights from this map to provide *evidence-based recommendations* for potentially optimizing ZLT's *default* active rule configurations (e.g., disabling a `pylint` rule if the map consistently shows `ruff` provides equal or better detection across all known samples).

**Rationale:**
*   **Prioritizes Detection:** Reduces the risk of "false negatives" (missed violations) compared to a strict initial whitelist approach.
*   **Manages Complexity Internally:** Shifts the burden of handling tool overlap and result correlation onto ZLT, simplifying the experience for the end-user/AI developer.
*   **Evidence-Based Optimization:** Avoids premature optimization or assumption-based configuration. Changes to the default active ruleset are driven by data gathered over time by `ZLT-dev`.
*   **Continuous Evolution:** Creates a mechanism for ZLT's configuration strategy to adapt as consultant tools evolve.

**Impact:**
*   Increases the initial implementation complexity for ZLT (aggregation logic).
*   Requires development of the `ZLT-dev` process and capability mapping database.
*   Leads to a potentially more robust detection mechanism in the near term and an adaptable, evidence-based configuration in the long term.
*   `TODO.md` and `ZerothLawAIFramework.py313.md` have been updated to reflect this strategy.

### 4.14 Input Robustness Verification (via Fuzz Testing)
*(Supports Principle #14)*

*   **Purpose:** To ensure modules handling complex or untrusted data are resilient against unexpected, malformed, or potentially malicious inputs that might not be covered by standard TDD/DDT test cases. Fuzzing acts as a stress test for input processing logic.
*   **Requirement Trigger:** ZLF **requires** fuzz testing for modules that:
    *   Parse complex file formats (e.g., configuration files beyond simple key-value, data serialization formats like custom binary protocols, source code).

## Naming Convention: Project Root vs. Package Directory (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ - AI: Run `date --iso-8601=seconds`)

**Problem:** The common Python convention `project/src/project/` leads to ambiguity and communication issues (especially with AI assistants) when differentiating between the project root directory and the Python package directory.

**Decision:** The ZLF mandates that the project root directory name and the primary Python package directory name (within `src/`) **must not** be identical.

The **recommended and adopted convention** for ZLF projects is:
`project_pkg/src/project/`

*   **Project Root:** `project_pkg` (e.g., `zeroth_law_pkg`)
*   **Package Directory:** `project` (e.g., `zeroth_law`)
*   **Imports:** Remain clean (e.g., `from zeroth_law import cli`)

**Rationale:**
*   Provides clear, unambiguous distinction between the containing project directory (`_pkg` suffix) and the importable Python package.
*   Prioritizes a clean, standard import name (`project`), which is used frequently in code and expected by tooling.
*   The suffixed root name (`project_pkg`) is explicit for configuration and navigation.
*   Avoids platform-specific issues (like case sensitivity) and uses standard characters.

**Impact:**
*   Requires renaming existing project root directories (e.g., `zeroth_law` -> `zeroth_law_pkg`).
*   Tooling configurations (like `pyproject.toml`) need to be aware that the package `project` resides in `src/` within the `project_pkg` root.
*   The ZLF document (`frameworks/python/ZerothLawAIFramework.py313.md`) has been updated to reflect this mandate and recommendation in Section 6.3.

## Pre-Commit Hook Environment Issue for MyPy in Monorepo (2025-04-11T01:06:32+08:00)

**Problem:** When running `mypy` via the project-specific `.pre-commit-config.yaml` (invoked by the custom multi-project dispatcher hook in the Git root), the execution environment set up by `pre-commit` caused persistent failures in `mypy`'s module path resolution for the `src`-layout (`src/zeroth_law/`). This occurred despite the dispatcher script successfully changing the working directory to the project root (`zeroth_law_pkg/`). Manifestations included recurring `"Source file found twice"` errors or failures to find `pyproject.toml` or source directories, depending on the specific hook configuration (`language: python` vs `language: system`, different `entry` commands). This indicates an issue with how the hook execution environment (paths, CWD inheritance, command resolution) interacts with `mypy` in this monorepo setup, rather than a fundamental `mypy` bug or configuration error in `pyproject.toml`.

**Failed Attempts (Illustrating Environment Issues):**
*   Using `language: python` with various `args`: Could not reliably resolve paths relative to the CWD set by the dispatcher, often hitting "Source file found twice" or "Cannot find file" errors.
*   Using `language: system` with `entry: poetry run mypy .`: Failed because the nested `poetry run` appeared to lose the CWD context set by the dispatcher.
*   Using `language: system` with `entry: mypy .` (with or without `--config-file`): Failed because `mypy` appeared to execute relative to the original Git root CWD, not the one set by the dispatcher.
*   Using `language: system` with `entry: sh -c 'cd zeroth_law_pkg && mypy . --config-file ./pyproject.toml'`: *Still* failed with "Source file found twice", indicating the subshell environment was also inheriting problematic path resolution or command execution context.
*   Standard `mypy` config adjustments in `pyproject.toml` (`mypy_path`, `explicit-package-bases`, `ignore_errors`) and environment variables (`MYPYPATH`) were ineffective against the hook execution environment issue.

**Working Solution (Hook Execution Workaround):**
The only configuration found to reliably execute `mypy` correctly *within the hook context* involves forcing a specific execution sequence using `language: system`:

```yaml
# In zeroth_law_pkg/.pre-commit-config.yaml
-   repo: local
    hooks:
      - id: mypy
        name: mypy (local project)
        language: system
        # The initial 'echo' appears necessary to establish a stable/correct
        # execution environment for the subsequent 'sh -c' command within the
        # pre-commit + language: system context on this specific setup.
        # It seems to allow 'sh' and its subsequent commands ('cd', 'mypy')
        # to resolve correctly relative to PATH and the CWD.
        entry: echo "Running mypy in zeroth_law_pkg" && sh -c 'cd zeroth_law_pkg && mypy . --config-file ./pyproject.toml'
        pass_filenames: false
        args: []
```

**Rationale for Workaround:** Adding the `echo` *before* the `sh -c '...'` command seems to influence the hook execution environment provided by `pre-commit` when using `language: system`. This allows the subsequent subshell (`sh -c`) to correctly resolve `sh`, `cd`, and `mypy` relative to the PATH and the CWD set by the dispatcher (`zeroth_law_pkg`). Without the preceding `echo`, the environment appeared inconsistent, potentially misinterpreting command paths or CWD. This workaround specifically addresses the hook execution environment quirk encountered in this setup.

## Stable Branch Vision vs. Development Practicality (2025-04-11T09:57:24+08:00)

**Goal:** The ultimate aim remains a `main` branch where the Zeroth Law Framework (ZLF) is strictly enforced, potentially treating all warnings from consultant tools (`ruff`, `mypy`, `pylint`, etc.) as errors. This state relies on the Zeroth Law Tool (ZLT) being mature enough to orchestrate all checks, aggregate results, and provide unified feedback.

**Current Reality:** ZLT is not yet capable of fulfilling this comprehensive orchestration role. Attempting to enforce the strict `main` standard manually or with current pre-commit hooks alone creates excessive friction and hinders development flow, contradicting ZLF principles.

**Decision & Rationale:** As a practical, temporary measure, a `dev` branch has been created from `main`. All active development will occur on `dev` with relaxed standards to facilitate progress until ZLT matures. The "warnings as errors" philosophy is suspended for non-critical issues during this phase on the `dev` branch. The `main` branch is reserved for the future ZLT-enforced state.

**Current Implementation (`dev` branch):**
*   **`pyproject.toml` Adjustment:** The `[tool.ruff.lint]` configuration has been modified:
    *   `select = ["E", "F"]`: Only critical Pyflakes error categories (E, F) are selected for `ruff check`, significantly reducing the number of issues flagged during pre-commit.
    *   `ignore = [...]`: A comprehensive list of specific warning/style rule codes has been added to `ignore` to further minimize noise from less critical checks.
*   **Goal Preservation:** This relaxation is explicitly temporary and applies to the `dev` branch. The long-term goal of a ZLT-enforced `main` branch remains unchanged. The focus is on unblocking development while ZLT evolves.
*   **Formatting:** Consistent formatting is still enforced by `ruff format` via IDE format-on-save, with the `pre-commit` hook acting as a final safety net.

## Finalized tool_mapping.yaml Structure (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ - AI: Run date --iso-8601=seconds)

**Problem:** Previous attempts to structure `option_mappings` in `tool_mapping.yaml` using nested dictionaries or lists of dictionaries failed due to `PyYAML` parsing errors (`mapping values are not allowed here`), despite the structures appearing valid according to the YAML specification. This blocked the goal of creating a rich mapping for a unified CLI interface.

**Goal:** Define a YAML structure that:
1.  Is reliably parsed by `PyYAML`.
2.  Clearly defines the unified options presented by `zlt` for a given action (the "ZLT Interface").
3.  Maps these unified `zlt` options to the specific command-line arguments required by each underlying "consultant" tool.
4.  Implicitly acts as a capability map, showing which tools support which conceptual `zlt` options.
5.  Is information-rich, allowing definitions of option types, descriptions, etc.

**Solution: Separated `zlt_options` and `maps_options`**

```yaml
# Action Definition (e.g., format)
format:
  description: Description of the ZLT action.

  # 1. Define the ZLT unified interface concepts for this action
  zlt_options:
    # Key: Conceptual option name used in zlt CLI (e.g., --quiet)
    quiet:
      # Value: Mapping defining the ZLT option's properties
      type: flag # Type (flag, value)
      # short: -q # Optional: Short flag name for CLI
      description: Unified description for this ZLT option.
    config:
      type: value
      value_type: path # Optional: Hint for CLI type
      description: Unified description.
    # Special entry for positional paths
    paths:
      type: positional
      # default: ["src"] # Optional list of default paths
      description: Unified description for paths.
    # ... other conceptual ZLT options for this action ...

  # 2. Define tools implementing this action
  tools:
    # Key: Unique name for the tool entry
    ruff_format:
      command: ruff format # Base command for the tool
      # Value: Mapping defining the tool and its specific option translations
      maps_options:
        # Key: The name of the option from 'zlt_options' above (e.g., quiet)
        # Value: The specific argument string the *tool* expects (e.g., -q)
        quiet: -q
        config: --config
        paths: null # Indicates tool uses positional paths defined in zlt_options.paths
        # Note: If 'verbose' existed in zlt_options but isn't listed here,
        #       it means ruff_format doesn't support zlt's verbose concept.
    black:
      command: black
      maps_options:
        quiet: --quiet # Black uses a different flag for the same concept
        config: --config
        paths: null
        # ... potentially other mappings specific to black ...
```

**Rationale:**
*   **Parsability:** This structure uses standard nested mappings (`zlt_options`) and simple key-value mappings (`maps_options`) in ways that `PyYAML` consistently handles, avoiding the previous parsing errors.
*   **Unified Interface:** `zlt_options` clearly defines the public-facing options for each `zlt` action.
*   **Translation Layer:** `maps_options` provides the explicit mapping from the `zlt` concept to the tool's specific argument string.
*   **Capability Map:** The presence or absence of a key in `maps_options` indicates whether a specific tool supports that conceptual `zlt` option.
*   **Richness:** `zlt_options` holds the detailed metadata (`type`, `description`, `value_type`).

**Implementation:** This requires refactoring `cli.py` to build Click options from `zlt_options` and `action_runner.py` to use `maps_options` for translating arguments during execution.

## Mandate: Minimize Mocking in Tests (2025-04-12T11:03:18+08:00)

**Decision:** To ensure tests accurately reflect the behavior of the system and its integrated components, the use of mocking (`unittest.mock`, `pytest-mock`) must be strictly limited.

**Mandate:**
1.  **No Mocking Internal Components:** Tests **must not** mock functions, classes, or modules that are part of the `zeroth_law` codebase itself (e.g., `cli.py`, `config_loader.py`, `action_runner.py`, `analyzer/`). Tests should interact with these components through their public APIs or by invoking the CLI.
2.  **No Mocking Accessible External Tools:** Tests **must not** mock the execution (`subprocess.run`) of external tools that are directly accessible and configurable (e.g., `ruff`, `mypy`, `pytest`). Tests requiring interaction with these tools should set up necessary configurations (e.g., temporary `pyproject.toml`, source files) and invoke the actual tools (e.g., via the appropriate `zlt` action command).
3.  **Permitted Mocking:** Mocking is **only permitted** for:
    *   Simulating truly external systems or dependencies (e.g., network services, databases - not currently applicable to ZLT).
    *   Controlling specific environmental conditions that are difficult or impossible to replicate reliably (e.g., specific filesystem states using `Path` method mocks like `is_dir`, `exists`; specific non-deterministic behavior like random number generation).
    *   Testing specific error handling paths for external processes (e.g., mocking `subprocess.run` *only* to simulate a non-zero exit code or specific error output from an external tool, while the success path uses the real tool).

**Rationale:**
*   **Test Reality:** Ensures tests validate the actual integration and behavior of the codebase, reducing the risk of tests passing while the real application fails.
*   **Refactoring Confidence:** Provides greater confidence when refactoring, as tests are coupled to behavior rather than implementation details hidden by mocks.
*   **Maintainability:** Reduces the fragility associated with complex mock setups that can break easily during refactoring.

**Impact:**
*   Requires reviewing and refactoring existing tests (identified in `test_cli.py`, `test_cli_json_output.py`, `test_cli_option_validation.py`) to remove inappropriate mocks.
*   May increase test setup complexity (e.g., creating temporary file structures and configurations).
*   Leads to more robust and reliable tests that provide higher confidence in the system's correctness.

## ZLF Exception: Using JSON for Machine-Generated Data Artifacts (2025-04-12T15:00:00+08:00)

**Context:** The ZLF generally prefers YAML over JSON due to YAML's support for comments, which aligns with Principle #7 (Self-Documenting Code & Explicit Rationale). This is important for configuration files or other formats intended for human/AI understanding and maintenance.

**Specific Case:** The strategy for verifying tool interfaces involves generating baseline data from command `--help` output. This process uses script(s) (likely `src/zeroth_law/dev_scripts/generate_baseline_cli.py` and associated logic) to capture help text, normalize it (via `| cat`), calculate CRC32 hashes (overall and per-line), and store this information.

**Decision:** For this *specific type of data artifact* (machine-generated, machine-read baseline data for automated tests), JSON is deemed an acceptable format, constituting a justified exception to the general preference for YAML.

**Rationale:**
*   **Machine-Centric:** The primary consumer of this file is the automated test suite, not a human or AI maintainer reading it for configuration or logic understanding.
*   **No Embedded Rationale Needed:** The *purpose* and *structure* of this data are defined by the generating script and the consuming test logic, not by comments within the file itself. The rationale for the *approach* is documented elsewhere (e.g., `NOTES.md`, commit history).
*   **Robustness & Simplicity:** JSON offers robust handling of text data (including potential special characters in help output via standard escaping) and simpler, less error-prone parsing compared to YAML for purely data-transfer purposes.

**Conclusion:** While YAML remains the preferred format for human/AI-editable configuration within the ZLF, JSON is permitted for machine-generated data artifacts where embedded comments are unnecessary and JSON's parsing simplicity and robustness are advantageous.

## Tool Definition Help Validation Refactoring & Enhancement (2025-04-12T19:19:13+08:00)

**Note:** The specific implementation details regarding normalization and validation logic described below are **superseded** by the workflow outlined in the "Current Workflow: Tool Help Baseline Generation & Verification" section (dated 2025-04-13). However, the problems identified and the rationale for refactoring remain relevant.

**Problem:** Initial tests validating tool definition YAMLs against command `--help` output using CRC32 hashes were brittle and failed inconsistently.
*   Failures occurred due to differences in terminal width affecting line wrapping in help text (`mypy` was particularly problematic).
*   Significant code duplication existed across test files (`tests/test_tool_defs/`).
*   The initial validation logic was complex and potentially incomplete.

**Solution & Rationale (Historical Context):**
1.  **Refactor to `conftest.py`:** Moved common helper functions (`get_command_output`, etc.), constants, and fixtures (`tool_definition`, `baseline_crc_data`) into `tests/test_tool_defs/conftest.py` to eliminate duplication. Test files now import helpers from `conftest`.
2.  **YAML Updates:** Added the `ignored_help_line_crc32s: []` key to relevant tool definition YAMLs and populated them with initial boilerplate/header CRCs.
3.  **Test Cleanup:** Removed redundant or unreliable older tests.

**Result:** The refactoring improved test structure and maintainability, paving the way for the current, more robust verification approach.

## ZLF Principle Tagging Strategy for Tests (2025-04-12T20:03:10+08:00)

**Goal:** Establish a clear and robust mechanism for associating test cases with the specific Zeroth Law Framework (ZLF) principles they aim to validate or exercise. This association is crucial for the `ZLT-dev` capability mapping process.

**Decisions (Based on AI Discussion):**

1.  **Tagging Mechanism (Decorator vs. Comment):**
    *   **Primary Method:** `@zlf_principle([...])` decorator. Chosen for robustness, AST parsability, validation potential, and future extensibility. A placeholder decorator (no runtime logic initially) will be defined.
    *   **Supported Alternative:** `# ZLF: [...]` structured comments. Allowed for lower friction, legacy cases, or simpler environments.
    *   **Precedence:** If both are present on the same element, the decorator's value is used.

2.  **Tagging Granularity & Inheritance:**
    *   **Module Level:** `# ZLF_MODULE: [...]` comment at the top of the file sets a default for all tests within.
    *   **Class Level:** `@zlf_principle([...])` decorator on a test class sets defaults for its methods.
    *   **Function Level:** `@zlf_principle([...])` decorator on a test function provides the most specific tag.
    *   **Inheritance:** Function > Class > Module. Tags at lower levels override or extend (policy TBD, likely override) tags from higher levels. This minimizes boilerplate while maximizing specificity.

3.  **Handling Multiple Principles:**
    *   **Allowed:** Tests can be tagged with multiple ZLF principles.
    *   **Primary Intent:** The *first* principle listed in the tag array is considered the primary intent the test is designed to validate. Subsequent principles represent collateral coverage.
    *   **Example:** `@zlf_principle(["#12", "#6"])` - Primary focus on Clarity (#12), also covers Complexity (#6).

4.  **Parameterized Tests (`@pytest.mark.parametrize`):**
    *   **Tagging:** The `@zlf_principle` tag is applied to the base test function definition.
    *   **Interpretation:** The tag applies equally to all test instances generated by the parametrization. Per-parameter tagging is deemed overly complex for current needs.

**Rationale:** This combined approach balances the need for a robust, parsable standard (decorators) with flexibility (comments, inheritance) and provides clear conventions for handling multiple principles and parametrization, supporting the goals of `ZLT-dev` capability mapping.

## Adapting TDD/DDT for External/Generated Data & AI Integration (2025-04-13T13:30:44+08:00)

**Problem:** Applying pure TDD/DDT is challenging when dealing with external systems (like tool `--help` output) or when integrating AI analysis, as the "expected" state isn't fully determined a priori.

**Approach (ZLF Adaptation):**
1.  **Step 0 (Characterization/Baseline):** Generate the current, real-world data (e.g., using the baseline generation script(s) like `generate_baseline_cli.py` to capture `tool --help` output). This establishes the "source of truth" for the current external state.
2.  **Step 1 (Define Intent/Structure - YAML):** Define the *semantic structure* and *ZLT's understanding* of the tool/data in a configuration file (e.g., tool definition YAMLs in `src/zeroth_law/tools/`). This declares *what* we want ZLT to know.
3.  **Step 2 (Link Structure to Reality - Initial Mapping):** Manually or semi-automatically connect the YAML structure to the baseline data (e.g., map YAML options to specific line CRCs in the JSON baseline). This synchronizes ZLT's understanding with the initial reality.
4.  **Step 3 (Regression Test - Pytest):** Use automated tests (like the CRC validation tests) to *continuously verify* that ZLT's understanding (YAML) remains synchronized with the characterized reality (JSON baseline). These tests act as regression guards.

**Handling Changes:**
*   When a test fails, it signals desynchronization.
*   Regenerate the baseline data (Step 0).
*   Manually inspect the diff between the old/new baseline and the YAML definition.
*   If the external change was intentional, **update the YAML** (Step 2) to realign ZLT's understanding. The test *should* fail until this is done.
*   If the change was unintentional or breaks ZLT's core assumptions, the test has caught a valid regression.

**AI Integration:**
*   **Future Helper:** AI can assist in Step 2 (mapping YAML to baseline data) by performing semantic analysis of help text.
*   **Future Validator:** AI can act as an additional test layer, semantically comparing YAML definitions against baseline content.
*   **Core ZLT:** The validated YAML definitions become the knowledge base for ZLT's AI-driven orchestration and configuration of tools.

**Conclusion:** TDD/DDT adapts by using characterization tests and baselines. The "test-first" focus applies to the *structure* and *validation logic*, while the external data is captured and used for ongoing regression testing. AI enhances this by adding semantic understanding and validation layers.

## Lessons Learned / Dead Ends (Help Validation)

*   **Normalization via `COLUMNS=32767`:** An early attempt to fix inconsistent line wrapping in `--help` output involved setting the `COLUMNS` environment variable to a very large value before capturing output. While helpful, this wasn't universally effective and could still lead to subtle differences. The current approach of piping through `cat` (`sh -c 'command | cat'`) during capture proved more robust and simpler for removing ANSI codes and ensuring consistent line breaks.
*   **Complex Validation Logic:** The previous `pytest` validation involved multiple steps: checking overall hash, then checking for missing expected CRCs, then calculating set differences to find unexpected CRCs. While functional, this was complex. The current approach simplifies this to a direct comparison of the complete CRC sets (`json_crc_set == yaml_crc_set`), which is easier to understand and implement, providing clearer failure modes (missing vs. stale CRCs).

## Mandate: Standardize on `uv` as the Primary Environment/Package Manager (2025-04-13T18:25:00Z) # AI: Run date --iso-8601=seconds

**Decision:** The Zeroth Law Framework (ZLF) will standardize on **`uv`** as the **primary and recommended** tool for Python environment management and package installation for projects adhering to the ZLF.

**Rationale:**
*   **ZLF Opinionation:** Aligns with the ZLF goal of being opinionated about core tooling to ensure consistency and reduce friction.
*   **Standards Alignment (PEP 621):** `uv` natively uses the standard `[project]` table in `pyproject.toml` for dependency specification, promoting interoperability and adherence to modern Python packaging standards.
*   **Performance:** `uv` offers significant speed advantages over Poetry for dependency resolution and installation.
*   **Modern Tooling:** Represents a current and actively developed tool focused on performance and standards compliance.

**Legacy/Transitional Support for Poetry:**
*   Poetry is now considered **legacy/transitional** within the ZLF context.
*   Projects migrating to ZLF may still use Poetry initially.
*   `uv` provides compatibility by being able to install dependencies from an existing `poetry.lock` file via `uv pip sync --resolver=poetry poetry.lock`. This offers a gradual migration path.
*   New ZLF-native projects **must** use `uv` and define dependencies in the `[project]` table.

**Impact:**
*   The ZLF documentation (`ZerothLawAIFramework.py313.md`) will be updated to reflect `uv` as the standard.
*   `pyproject.toml` examples and requirements will use the `[project]` table for dependencies.
*   Scripts and tests interacting with the environment manager (e.g., baseline generation scripts like `generate_baseline_cli.py`, `test_tool_integration.py`) must be updated to use `uv` commands or operate correctly within a `uv`-managed environment.
*   CI/CD workflows must be updated to use `uv` for environment setup and dependency installation.
*   `TODO.md` will be updated with specific migration tasks.

## Refined Tool Interface Workflow & AI Interpretation (v3) (YYYY-MM-DDTHH:MM:SS+ZZ:ZZ) # AI: Run `date --iso-8601=seconds`

**--- MANDATE ---**
**`tool_index.json` is 100% Deterministic:** The `src/zeroth_law/tools/tool_index.json` file is **solely** managed by automated scripts (specifically, the logic within `src/zeroth_law/dev_scripts/baseline_generator.py::generate_or_verify_baseline` as executed by tests). It serves as a cache reflecting the state (CRC, timestamps) of the ground-truth `.txt` baseline files.
**NEVER Edit Manually or via AI:** Under **no circumstances** should `tool_index.json` be directly modified by human intervention or AI editing tools during the standard development or testing workflow. Any direct edits violate this mandate and will likely be overwritten or cause inconsistencies.
**AI Role is Interpretation Only:** The AI's responsibility in this workflow is strictly limited to interpreting the ground-truth `.txt` files and populating/updating the corresponding `.json` *definition* files (e.g., `src/zeroth_law/tools/<tool>/<id>.json`). This includes syncing the `metadata.ground_truth_crc` within the `.json` definition file to match the value found in the programmatically managed `tool_index.json`.
**--- END MANDATE ---**

**--- Clarification (2025-04-20T16:07:17+08:00) ---**
To reiterate the mandate above for absolute clarity: The ONLY non-deterministic task permitted within this tool definition workflow is the AI's interpretation of a tool's unstructured `.txt` help output into the structured `.json` definition format. All other aspects, especially the generation and management of CRC values within `tool_index.json`, MUST be handled deterministically by the project's automated scripts (primarily invoked via `pytest`). Any deviation from this is a violation of the workflow.
**--- END Clarification ---**

**--- FURTHER MANDATE (JSON Interpretation - 2025-04-20T16:14:29+08:00) ---**
**ABSOLUTELY NO BACKSLASHES (`\`)**: When interpreting `.txt` help text into `.json` definition files, the use of backslashes (`\`) for any purpose, including escaping characters within strings (e.g., `\'`, `\"`, `\\`), is **strictly forbidden**.
**FORBIDDEN BY PROXY (2025-04-20T16:15:02+08:00):** If a character in the source `.txt` help text requires a backslash escape for correct representation within a standard double-quoted JSON string, that character itself is **forbidden by proxy** and must not be included. Handle this by rephrasing the text to avoid the character or omitting the problematic phrase entirely. Backslashes and characters requiring them introduce parsing ambiguity and errors and must be avoided.
**--- END FURTHER MANDATE ---**

**Summary:** This section details the current, streamlined workflow for managing external tool interface definitions, emphasizing the separation between automated ground-truth tracking and AI-driven interpretation. This supersedes previous workflows involving YAML or complex per-line CRC checks.

**Workflow Steps:**

1.  **Tool Discovery & Indexing (Tests - `test_ensure_txt_baselines_exist.py`):**
    *   The test function `get_managed_sequences` examines the environment's executables against whitelists/blacklists in `pyproject.toml`.
    *   It determines the managed tool sequences (base commands and discovered subcommands).
    *   It fails the test run if orphaned tools are found or whitelisted tools are missing.

2.  **Ground Truth Check & Update (Core logic in `src/zeroth_law/dev_scripts/baseline_generator.py::generate_or_verify_baseline`, called by tests):
    *   For each managed sequence, captures `command --help | cat` output.
    *   Calculates the CRC32 hash of the output.
    *   Compares calculated CRC with the `crc` stored in `tool_index.json` for that sequence.
    *   **If CRCs match:** Updates only the `checked_timestamp` in `tool_index.json`.
    *   **If CRCs mismatch (or entry is new):**
        *   Overwrites the corresponding `.txt` file (`src/zeroth_law/tools/<tool>/<id>.txt`) with the new output.
        *   Updates `crc`, `updated_timestamp`, and `checked_timestamp` in `tool_index.json`.
        *   **Ensures Minimal Skeleton JSON:** Checks if `src/zeroth_law/tools/<tool>/<id>.json` exists. If not, creates it with basic keys and `metadata.ground_truth_crc = "0x00000000"`. It *does not* modify an existing JSON or populate the skeleton.

3.  **AI Interpretation & Synchronization (`.json` Population):**
    *   Triggered by a failing `test_txt_json_consistency.py` (see Step 4).
    *   **Me (the AI)** reads the existing `.json` file (which might be a skeleton or partially populated).
    *   **Me (the AI)** reads the ground-truth `.txt` file (which was updated in Step 2 if necessary).
    *   **Me (the AI)** holistically compares the `.txt` content to the `.json` structure. Update the `.json` content (filling `description`, `options`, `arguments`, `subcommands` etc.) based on the `.txt` content and schema guidelines, ensuring the entire JSON structure accurately reflects the interpretation of the TXT baseline. Preserve existing unchanged elements where appropriate.
    *   **Me (the AI)** **ONLY AFTER** ensuring the JSON content is fully updated and consistent with the `.txt` interpretation, update the `metadata.ground_truth_crc` field *inside the `.json` file* to match the authoritative `crc` value from `tool_index.json`. **Updating the CRC in isolation without verifying/updating the content first is incorrect procedure.**

4.  **Verification (Separate Tests):
    *   `test_txt_json_consistency.py`:
        *   Compares the `metadata.ground_truth_crc` value *inside* the `.json` file against the `crc` value stored in `tool_index.json` for the same tool ID.
        *   A mismatch indicates the AI interpretation (Step 3) is needed or incomplete.
    *   `test_json_schema_validation.py`:
        *   Validates the structure and content rules of the populated `.json` files against the defined schema.

**Rationale:** This workflow uses automated tests for discovery and ground-truth (`.txt`, index CRC) updates. It relies on the AI for the interpretation step (`.json` content population and internal CRC sync), clearly signaled by the `test_txt_json_consistency.py` failure.

**Consumption by ZLT:** The AI-populated and validated `.json` file serves as the machine-readable knowledge base for ZLT.

# Process for Populating Tool Definition JSON Files

1.  **Run Baseline Tests:**
    *   Execute `uv run pytest tests/test_tool_defs/`
    *   This automatically runs `test_ensure_txt_baselines_exist.py` which calls `generate_or_verify_baseline`.
    *   This ensures all `.txt` files and `tool_index.json` CRCs/timestamps are up-to-date, and skeleton `.json` files exist.
2.  **Identify AI Task Triggers:**
    *   Look for failures specifically in `test_txt_json_consistency.py::test_json_crc_matches_index`.
    *   A failure here means the `metadata.ground_truth_crc` inside a `.json` file is outdated (likely `0x00000000` or an old value) compared to the `tool_index.json`.
    *   Also note failures in `test_json_schema_validation.py` which indicate structural issues in the JSON content itself.

3.  **Iterate Through Failing Files (AI Task):**
    *   For each tool ID failing `test_txt_json_consistency.py` or `test_json_schema_validation.py`:
        *   **Locate Files:**
            *   JSON: `src/zeroth_law/tools/<tool>/<id>.json`
            *   Help Text: `src/zeroth_law/tools/<tool>/<id>.txt`
            *   Index: `src/zeroth_law/tools/tool_index.json`
        *   **Read Existing JSON:** Use `read_file` on the `.json` file.
        *   **Read Help Text:** Use `read_file` on the `.txt` file.
        *   **Get Index CRC:** Extract the authoritative `crc` for the tool ID from `tool_index.json`.
        *   **Populate/Correct JSON:**
            *   Use `edit_file` on the `.json` file.
            *   Carefully compare `.txt` to the existing JSON structure.
            *   Incrementally fill/update `description`, `usage`, `options`, `arguments`, `subcommands` based on the `.txt` help text and schema guidelines (`tools/zlt_schema_guidelines.md`), preserving unchanged elements.
            *   **Crucially:** Update `metadata.ground_truth_crc` within the JSON *with the correct CRC value extracted from the index*.
            *   Ensure the final JSON structure adheres to the schema (`test_json_schema_validation.py` failures provide guidance).
        *   **Handle Edit Failures:** If `edit_file` reports "no changes" but `pytest` still fails the file, use `reapply` on the target `.json` file.
        *   **Verify:** Rerun `uv run pytest tests/test_tool_defs/` to confirm the test(s) for the specific tool ID now pass.

4.  **Repeat:** Continue until all tests in `tests/test_tool_defs/` pass.

### Testing Techniques: Using External Helper Scripts as Test Doubles

**Context:** When integration testing components that interact with external command-line tools or processes (e.g., via `subprocess`), directly mocking the `subprocess` call or internal functions can hide integration issues and violate the project's strict no-monkeypatching rule. Embedding command logic or output as strings within test code is also discouraged due to fragility, especially regarding escape characters.

**Technique:**

Instead of mocking or embedding, use dedicated external helper script files as "Test Doubles" for the real external commands:

1.  **Create Simple Scripts:** For each distinct output or behavior needed from an external command during a test, create a minimal, executable script file (e.g., a Python script using `sys.stdout.write`) that produces exactly that output or simulates that behavior.
2.  **Store in Test Data:** Place these helper scripts in a dedicated test data directory (e.g., `tests/test_data/helper_scripts/`).
3.  **Copy & Execute in Test:** Within the test function:
    *   Use a helper function to get the path to the required test data script.
    *   Use `shutil.copy` to copy the script into the test's temporary directory (`tmp_path`).
    *   Make the copied script executable (`os.chmod`).
    *   Construct the command to execute this *copied* script (e.g., `f"{sys.executable} {copied_script_path}"`).
    *   Pass this command to the function under test that invokes the external process.
4.  **Assert:** Verify the results based on the *known, controlled output* of the helper script.

**Benefits:**

*   **No Forbidden Monkeypatching:** Avoids patching `subprocess` or internal code.
*   **Robust Integration Test:** Tests the actual process invocation, execution, and output capturing logic of the code under test.
*   **Avoids Escape Character Issues:** Script content is stored cleanly in its own file, eliminating problems with embedding code/output strings in test files.
*   **Clear Separation:** Keeps test logic clean and separates the definition of the external dependency's behavior (in the script file) from the test's orchestration code.

**Limitation:** This primarily tests the interaction with the process based on its *output*. It may not fully replicate complex exit code behaviors or side effects of the *real* external tool unless explicitly coded into the helper script. It also requires careful management of the helper script files.
