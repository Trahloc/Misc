#!/usr/bin/env python3
"""
pipreqs - Python Requirements Manager

A tool for managing requirements.txt files across multiple Python projects.
Features:
- Installs requirements from multiple subdirectories
- Caches downloaded packages to avoid repeated downloads
- Aggregates requirements into a single comprehensive file
- Detects and resolves version conflicts
- Validates package dependencies

Usage:
  pipreqs [OPTIONS]

Options:
  -r, --requirements FILE     Specify base project requirements file
  -e, --extensions DIR        Directory to search for requirements.txt files
  -a, --aggregate FILE        Output aggregated requirements file (default: pipreqs.txt)
  -c, --cache DIR             Custom cache directory (default: ~/.cache/pipreqs)
  -u, --upgrade               Upgrade packages to latest versions
  -v, --verbose               Enable verbose output (use -vv for even more detail)
  -d, --dry-run               Show what would be done without making changes
  --install                   Install packages after processing (default behavior)
  --no-install                Skip installation, only aggregate requirements
  --no-resolve-conflicts      Do not attempt to resolve version conflicts

Examples:
  pipreqs -e extensions_folder -r project_requirements.txt
  pipreqs -e src/plugins -v --upgrade
  pipreqs -e extensions -r base_requirements.txt -a aggregated.txt --no-install
"""

import argparse
import hashlib
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import urllib.request
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# ANSI color codes for terminal output
COLORS = {
    "reset": "\033[0m",
    "red": "\033[31m",
    "green": "\033[32m",
    "yellow": "\033[33m",
    "blue": "\033[34m",
    "magenta": "\033[35m",
    "cyan": "\033[36m",
    "bold": "\033[1m"
}

@dataclass
class Package:
    """Represents a Python package with name and version constraint."""
    name: str
    version_constraint: str = ""
    source_file: str = ""
    index_url: str = ""  # Store the index URL context
    
    @property
    def qualified_name(self) -> str:
        """Return the qualified package name with version constraint."""
        if self.version_constraint:
            return f"{self.name}{self.version_constraint}"
        return self.name
    
    def __str__(self) -> str:
        return self.qualified_name

@dataclass
class CachedFile:
    """Represents a cached file with metadata."""
    url: str
    local_path: Path
    etag: Optional[str]
    last_modified: Optional[str]
    timestamp: datetime
    size: int

class PipReqsManager:
    """Manage Python requirements across multiple directories."""
    
    # ANSI color codes for terminal output
    COLORS = {
        "reset": "\033[0m",
        "red": "\033[31m",
        "green": "\033[32m",
        "yellow": "\033[33m",
        "blue": "\033[34m",
        "magenta": "\033[35m",
        "cyan": "\033[36m",
        "bold": "\033[1m"
    }

    # Verbosity levels
    VERBOSITY = {
        "minimal": 0,  # Just errors and critical info
        "normal": 1,   # Standard information (default)
        "verbose": 2,  # More detailed logs
        "debug": 3     # Full debug information
    }
    
    def __init__(self, args):
        self.args = args
        self.verbosity = self._get_verbosity_level(args.verbose)
        self.cache_dir = Path(args.cache).expanduser() if args.cache else Path.home() / ".cache" / "pipreqs"
        self.cache_meta_file = self.cache_dir / "meta.json"
        self.packages: Dict[str, Package] = {}
        self.requirements_files: List[Path] = []
        self.base_requirements: Optional[Path] = None
        self.cached_files: Dict[str, CachedFile] = {}
        self.total_bytes_saved = 0
        
        # Create cache directory if it doesn't exist
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._load_cache_metadata()
    
    def _get_verbosity_level(self, verbose_count):
        """Convert verbose count to verbosity level."""
        if verbose_count == 0:
            return self.VERBOSITY["normal"]
        elif verbose_count == 1:
            return self.VERBOSITY["verbose"]
        else:
            return self.VERBOSITY["debug"]
    
    def _load_cache_metadata(self):
        """Load cache metadata from disk."""
        if self.cache_meta_file.exists():
            try:
                with open(self.cache_meta_file, 'r') as f:
                    data = json.load(f)
                    for url, entry in data.items():
                        self.cached_files[url] = CachedFile(
                            url=url,
                            local_path=Path(entry['local_path']),
                            etag=entry.get('etag'),
                            last_modified=entry.get('last_modified'),
                            timestamp=datetime.fromisoformat(entry['timestamp']),
                            size=entry.get('size', 0)
                        )
                self.log(f"Loaded metadata for {len(self.cached_files)} cached files", level="debug")
            except Exception as e:
                self.log(f"Error loading cache metadata: {e}", level="error")
    
    def _save_cache_metadata(self):
        """Save cache metadata to disk."""
        data = {}
        for url, cached_file in self.cached_files.items():
            data[url] = {
                'local_path': str(cached_file.local_path),
                'etag': cached_file.etag,
                'last_modified': cached_file.last_modified,
                'timestamp': cached_file.timestamp.isoformat(),
                'size': cached_file.size
            }
        
        with open(self.cache_meta_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def log(self, message, level="info"):
        """Log a message with appropriate formatting based on verbosity level."""
        level_map = {
            "debug": self.VERBOSITY["debug"],
            "verbose": self.VERBOSITY["verbose"],
            "info": self.VERBOSITY["normal"],
            "warning": self.VERBOSITY["minimal"],
            "error": self.VERBOSITY["minimal"],
            "success": self.VERBOSITY["minimal"]
        }
        
        # Only log if the current verbosity level includes this message type
        if self.verbosity < level_map.get(level, 0):
            return
        
        color_map = {
            "info": COLORS["reset"],
            "debug": COLORS["blue"],
            "verbose": COLORS["cyan"],
            "warning": COLORS["yellow"],
            "error": COLORS["red"],
            "success": COLORS["green"]
        }
        
        color = color_map.get(level, COLORS["reset"])
        print(f"{color}[{level.upper()}] {message}{COLORS['reset']}")
    
    def find_requirements_files(self) -> List[Path]:
        """Find all requirements.txt files in the specified directory and add base requirements if specified."""
        files = []
        
        # Add base requirements file if specified
        if self.args.requirements:
            base_req_path = Path(self.args.requirements).expanduser()
            if not base_req_path.exists():
                self.log(f"Base requirements file {base_req_path} does not exist", level="error")
            else:
                self.log(f"Using base requirements file: {base_req_path}", level="info")
                self.base_requirements = base_req_path
                files.append(base_req_path)
        
        # Find requirements files in extensions directory
        if not self.args.extensions:
            self.log("No extensions directory specified, using only base requirements", level="warning")
            return files
        
        extensions_dir = Path(self.args.extensions).expanduser()
        if not extensions_dir.exists() or not extensions_dir.is_dir():
            self.log(f"Extensions directory {extensions_dir} does not exist or is not a directory", level="error")
            return files
        
        ext_requirements_files = list(extensions_dir.glob("**/requirements.txt"))
        self.log(f"Found {len(ext_requirements_files)} requirements.txt files in extensions directory", level="info")
        files.extend(ext_requirements_files)
        
        return files
    
    def parse_requirements_file(self, file_path: Path) -> List[Package]:
        """Parse a requirements.txt file and return a list of packages."""
        packages = []
        current_index_url = None  # Track the current index URL context
        
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                if not line or line.startswith('#'):
                    i += 1
                    continue
                
                # Handle pip command line options
                if line.startswith('--'):
                    parts = line.split(maxsplit=1)
                    option = parts[0]
                    
                    # Handle the case where the value is on the same line
                    if len(parts) > 1:
                        value = parts[1]
                        if option in ["--extra-index-url", "--index-url"]:
                            current_index_url = value
                            self.log(f"Using index URL context: {option} {value}", level="verbose")
                    # Handle the case where the value is on the next line
                    elif option in ["--extra-index-url", "--index-url"] and i + 1 < len(lines):
                        next_line = lines[i + 1].strip()
                        if next_line and not next_line.startswith(('--', '#')):
                            current_index_url = next_line
                            self.log(f"Using index URL context from next line: {option} {next_line}", level="verbose")
                            i += 1  # Skip the next line
                    
                    # For other pip options, don't change the current_index_url
                    i += 1
                    continue
                
                # Handle direct URL references
                if line.startswith(('http://', 'https://', 'git+', 'svn+', 'hg+')):
                    self.log(f"Direct URL requirement: {line}", level="verbose")
                    package = Package(name=line, source_file=str(file_path))
                    if current_index_url:
                        # Store the index URL context with the package
                        package.index_url = current_index_url
                    packages.append(package)
                    i += 1
                    continue
                
                # Handle regular package requirements
                match = re.match(r'^([a-zA-Z0-9_\-\.]+)(.*)$', line)
                if match:
                    name, version_constraint = match.groups()
                    package = Package(
                        name=name.lower(),
                        version_constraint=version_constraint.strip(),
                        source_file=str(file_path)
                    )
                    if current_index_url:
                        # Store the index URL context with the package
                        package.index_url = current_index_url
                    packages.append(package)
                    self.log(f"Package requirement: {name}{version_constraint}", level="debug")
                i += 1
        except Exception as e:
            self.log(f"Error parsing {file_path}: {e}", level="error")
            if self.verbosity >= self.VERBOSITY["debug"]:
                import traceback
                self.log(traceback.format_exc(), level="debug")
        
        return packages
    
    def aggregate_requirements(self, prioritized_requirements=None) -> Dict[str, Package]:
        """Aggregate requirements from all files and handle conflicts based on extension priority."""
        all_packages: Dict[str, List[Package]] = defaultdict(list)
        
        # Use provided priority order or default file order
        req_files = prioritized_requirements if prioritized_requirements else self.requirements_files
        
        # First pass: collect all package versions
        for req_file in req_files:
            packages = self.parse_requirements_file(req_file)
            for package in packages:
                if package.name.startswith(('http://', 'https://', 'git+')):
                    # For direct URLs, use URL as key
                    all_packages[package.name].append(package)
                else:
                    all_packages[package.name.lower()].append(package)
        
        # Second pass: resolve conflicts using priority order
        resolved_packages = {}
        conflicts = []
        
        for name, package_variants in all_packages.items():
            if len(package_variants) == 1:
                # No conflict
                resolved_packages[name] = package_variants[0]
            else:
                # Check for conflicts
                version_constraints = {p.version_constraint for p in package_variants}
                if len(version_constraints) == 1:
                    # Same version constraint across all occurrences
                    resolved_packages[name] = package_variants[0]
                else:
                    conflicts.append((name, package_variants))
                    
                    if prioritized_requirements:
                        # Use priority order to resolve conflict
                        # Get the highest priority package variant
                        highest_priority = None
                        for req_file in prioritized_requirements:
                            for variant in package_variants:
                                if Path(variant.source_file) == req_file:
                                    highest_priority = variant
                                    break
                            if highest_priority:
                                break
                        
                        if highest_priority:
                            resolved_packages[name] = highest_priority
                        else:
                            # Fallback if no match found (shouldn't happen)
                            resolved_packages[name] = self._get_most_restrictive(package_variants)
                            
                    elif self.args.resolve_conflicts:
                        if self.args.upgrade:
                            if all(not p.version_constraint or p.version_constraint.startswith('>') for p in package_variants):
                                resolved_packages[name] = self._get_least_restrictive(package_variants)
                            else:
                                resolved_packages[name] = self._get_most_restrictive(package_variants)
                        else:
                            resolved_packages[name] = self._get_most_restrictive(package_variants)
                    else:
                        resolved_packages[name] = package_variants[0]
                        self.log(f"Version conflict for {name}: {[p.qualified_name for p in package_variants]}", level="warning")
        
        if conflicts and self.verbosity >= self.VERBOSITY["minimal"]:
            conflicts_by_source = defaultdict(list)
            for name, variants in conflicts:
                for variant in variants:
                    conflicts_by_source[variant.source_file].append((name, variant))
            
            self.log(f"Detected {len(conflicts)} package version conflicts from {len(conflicts_by_source)} different sources", level="warning")
            
            if self.verbosity >= self.VERBOSITY["verbose"]:
                for source, package_conflicts in conflicts_by_source.items():
                    source_path = Path(source)
                    if self.base_requirements and source_path == self.base_requirements:
                        source_type = "base requirements"
                    else:
                        if self.args.extensions:
                            try:
                                ext_dir = Path(self.args.extensions).expanduser()
                                rel_path = source_path.relative_to(ext_dir)
                                source_type = f"extension: {rel_path}"
                            except ValueError:
                                source_type = f"file: {source_path}"
                        else:
                            source_type = f"file: {source_path}"
                    
                    self.log(f"  From {source_type}:", level="warning")
                    for name, variant in package_conflicts:
                        self.log(f"    - {name}{variant.version_constraint}", level="warning")
                
                self.log("\nResolutions (based on priority):", level="warning")
                for name, _ in conflicts:
                    chosen = resolved_packages[name]
                    source_path = Path(chosen.source_file)
                    if self.base_requirements and source_path == self.base_requirements:
                        source_type = "base requirements"
                    else:
                        if self.args.extensions:
                            try:
                                ext_dir = Path(self.args.extensions).expanduser()
                                rel_path = source_path.relative_to(ext_dir)
                                source_type = f"extension: {rel_path}"
                            except ValueError:
                                source_type = f"file: {source_path}"
                        else:
                            source_type = f"file: {source_path}"
                    self.log(f"  {name}: {chosen.qualified_name} (from {source_type})", level="warning")
        
        return resolved_packages

    def _resolve_conflict_interactively(self, package_name: str, variants: List[Package]) -> Package:
        """Interactively resolve a package version conflict."""
        print(f"\n{COLORS['bold']}CONFLICT RESOLUTION REQUIRED{COLORS['reset']}")
        print(f"Package '{package_name}' has conflicting version requirements:")
        
        # Display options with information about their source
        for i, variant in enumerate(variants, 1):
            source_path = Path(variant.source_file)
            if self.base_requirements and source_path == self.base_requirements:
                source_type = "base requirements"
            else:
                if self.args.extensions:
                    try:
                        rel_path = source_path.relative_to(Path(self.args.extensions).expanduser())
                        source_type = f"extension: {rel_path}"
                    except ValueError:
                        source_type = f"file: {source_path}"
                else:
                    source_type = f"file: {source_path}"
            
            print(f"{i}. {variant.qualified_name} from {source_type}")
        
        # Add automatic resolution options
        print(f"{len(variants) + 1}. Use most restrictive version (recommended for stability)")
        print(f"{len(variants) + 2}. Use least restrictive version (may cause compatibility issues)")
        
        # Get user choice
        choice = None
        while choice is None:
            try:
                user_input = input("\nEnter your choice (number): ")
                choice_num = int(user_input.strip())
                if 1 <= choice_num <= len(variants) + 2:
                    if choice_num <= len(variants):
                        choice = variants[choice_num - 1]
                    elif choice_num == len(variants) + 1:
                        choice = self._get_most_restrictive(variants)
                    else:
                        choice = self._get_least_restrictive(variants)
                else:
                    print(f"Please enter a number between 1 and {len(variants) + 2}")
                    choice = None
            except ValueError:
                print("Please enter a valid number")
        
        print(f"{COLORS['green']}Selected: {choice.qualified_name}{COLORS['reset']}\n")
        return choice

    def _get_most_restrictive(self, packages: List[Package]) -> Package:
        """Return the most restrictive version from a list of packages."""
        # Simple heuristic: packages with exact version (==) are most restrictive
        # followed by those with >= and > constraints
        for p in packages:
            if "==" in p.version_constraint:
                return p
        
        # For now, just return the one with the longest constraint as a simple heuristic
        return max(packages, key=lambda p: len(p.version_constraint))
    
    def _get_least_restrictive(self, packages: List[Package]) -> Package:
        """Return the least restrictive version from a list of packages."""
        # For upgrade mode, prefer packages with no constraint
        for p in packages:
            if not p.version_constraint:
                return p
        
        # Otherwise, prefer >= over == constraints
        for p in packages:
            if ">=" in p.version_constraint:
                return p
        
        # Default to first package
        return packages[0]
    
    def _download_and_cache_url(self, url: str) -> Optional[Path]:
        """Download a file from URL and cache it locally, return the path to cached file."""
        import hashlib
        
        # Create a filename based on URL hash
        url_hash = hashlib.sha256(url.encode()).hexdigest()
        file_ext = url.split("?")[0].split("#")[0].split("/")[-1]
        if not any(file_ext.endswith(ext) for ext in [".whl", ".tar.gz", ".zip", ".egg"]):
            # If no recognizable extension, add .whl as default
            file_ext += ".whl"
        
        cached_path = self.cache_dir / f"{url_hash}_{file_ext}"
        
        # Check if already cached
        if cached_path.exists():
            self.log(f"Using cached file for {url} at {cached_path}", level="verbose")
            return cached_path
        
        # Not cached, download it
        self.log(f"Downloading {url} to cache...", level="verbose")
        try:
            import urllib.request
            import urllib.error
            import shutil
            
            with urllib.request.urlopen(url) as response:
                content_type = response.info().get_content_type()
                if content_type not in ["application/zip", "application/x-tar", "application/x-gzip", "application/octet-stream"]:
                    self.log(f"Invalid content type {content_type} for URL {url}", level="error")
                    return None

                with open(cached_path, "wb") as out_file:
                    shutil.copyfileobj(response, out_file)
            
            # Get file size for stats
            file_size = cached_path.stat().st_size
            self.total_bytes_saved += file_size
            
            # Store metadata
            self.cached_files[url] = CachedFile(
                url=url,
                local_path=cached_path,
                etag=None,
                last_modified=None,
                timestamp=datetime.now(),
                size=file_size
            )
            self._save_cache_metadata()
            
            self.log(f"Downloaded {url} to {cached_path} ({self._format_size(file_size)})", level="debug")
            return cached_path
        except (urllib.error.URLError, urllib.error.HTTPError) as e:
            self.log(f"Error downloading {url}: {e}", level="error")
            if self.verbosity >= self.VERBOSITY["debug"]:
                self.log(f"URL: {url}", level="debug")
            if cached_path.exists() and cached_path.stat().st_size == 0:
                cached_path.unlink()  # Remove empty/partial file
            return None
    
    def write_aggregated_requirements(self, packages: Dict[str, Package]) -> bool:
        """Write aggregated requirements to file."""
        if not self.args.aggregate:
            self.log("No output aggregate file specified, skipping aggregation", level="warning")
            return False

        output_file = Path(self.args.aggregate).expanduser()

        try:
            # Create parent directories if they don't exist
            output_file.parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, 'w') as f:
                f.write("# Aggregated requirements file generated by pipreqs\n")
                f.write(f"# Generated on: {datetime.now().isoformat()}\n")
                f.write("# Sources:\n")

                if self.base_requirements:
                    f.write(f"#   {self.base_requirements} (base requirements)\n")

                for req_file in self.requirements_files:
                    # Skip base requirements to avoid duplication in the list
                    if self.base_requirements and req_file == self.base_requirements:
                        continue
                    f.write(f"#   {req_file}\n")
                f.write("\n")

                # Group packages by index URL
                packages_by_index = defaultdict(list)
                for name, package in packages.items():
                    packages_by_index[package.index_url].append(package)
                
                # Write packages grouped by index URL
                for index_url, pkg_list in packages_by_index.items():
                    # Write the index URL if it exists
                    if index_url:
                        f.write(f"--extra-index-url {index_url}\n")
                    
                    # Sort packages in this group
                    for package in sorted(pkg_list, key=lambda p: p.name):
                        # Handle direct URLs by caching them locally
                        if package.name.startswith(("http://", "https://", "git+")):
                            url = package.name
                            # Only try to cache http/https URLs, not git+
                            if url.startswith(("http://", "https://")) and not url.startswith("git+"):
                                # Download and cache the URL
                                cached_path = self._download_and_cache_url(url)
                                if cached_path:
                                    # Use file:// URL to the cached file
                                    f.write(f"file://{cached_path}\n")
                                    continue
                        
                        # For regular packages or if caching failed, use the original
                        f.write(f"{package.qualified_name}\n")
                    
                    # Add a blank line between index URL groups
                    f.write("\n")

            self.log(f"Aggregated {len(packages)} packages to {output_file}", level="success")
            return True
        except Exception as e:
            self.log(f"Error writing aggregated requirements file: {e}", level="error")
            if self.verbosity >= self.VERBOSITY["debug"]:
                import traceback
                self.log(traceback.format_exc(), level="debug")
            return False
    
    def install_packages(self, packages: Dict[str, Package]) -> bool:
        """Install packages using pip with caching."""
        if self.args.no_install:
            self.log("Installation skipped (--no-install specified)", level="info")
            return True
        
        if self.args.dry_run:
            self.log("Dry run - would install packages:", level="info")
            for name, package in sorted(packages.items()):
                self.log(f"  {package.qualified_name}", level="info")
            return True
        
        success = True
        
        try:
            # Group packages by index URL
            packages_by_index = defaultdict(list)
            for name, package in packages.items():
                packages_by_index[package.index_url].append(package)
            
            # Install each group separately
            for index_url, pkg_list in packages_by_index.items():
                temp_file = None
                try:
                    # Create a temporary requirements file for this group
                    fd, temp_path = tempfile.mkstemp(suffix='.txt', prefix='pipreqs-temp-')
                    temp_file = Path(temp_path)
                    os.close(fd)
                    
                    with open(temp_file, 'w') as f:
                        for package in sorted(pkg_list, key=lambda p: p.name):
                            f.write(f"{package.qualified_name}\n")
                    
                    # Set up download cache
                    os.environ['PIP_CACHE_DIR'] = str(self.cache_dir / "pip_cache")
                    
                    # Prepare pip command
                    pip_cmd = [sys.executable, "-m", "pip", "install", "-r", str(temp_file)]
                    
                    if self.args.upgrade:
                        pip_cmd.append("--upgrade")
                    
                    # Add index URL if specified for this group
                    if index_url:
                        pip_cmd.extend(["--extra-index-url", index_url])
                    
                    # Set up HTTP cache handler
                    self._setup_pip_download_cache()
                    
                    # Run pip
                    self.log(f"Installing packages with command: {' '.join(pip_cmd)}", level="verbose")
                    if index_url:
                        self.log(f"Installing packages from index {index_url}...", level="info")
                    else:
                        self.log(f"Installing packages...", level="info")
                    
                    # Use different capture approaches based on verbosity
                    if self.verbosity >= self.VERBOSITY["debug"]:
                        # For debug, just let pip output directly to console
                        process = subprocess.run(pip_cmd, check=False)
                    else:
                        # For other verbosity levels, capture output
                        process = subprocess.run(
                            pip_cmd,
                            capture_output=True,
                            text=True,
                            check=False
                        )
                        
                        # Show output based on verbosity level
                        if self.verbosity >= self.VERBOSITY["verbose"] and process.stdout:
                            self.log("pip stdout output:", level="verbose")
                            for line in process.stdout.splitlines():
                                self.log(f"  {line}", level="verbose")
                    
                    if process.returncode != 0:
                        self.log(f"pip install failed with exit code {process.returncode}", level="error")
                        if process.stderr:
                            self.log("pip stderr output:", level="error")
                            for line in process.stderr.splitlines():
                                self.log(f"  {line}", level="error")
                            
                            # Check for dependency conflicts
                            if "ResolutionImpossible" in process.stderr or "conflicting dependencies" in process.stderr:
                                self.log("\nDependency conflict detected. This might be caused by unresolved transitive dependencies.", level="error")
                                self.log("Consider using the --interactive flag for manual conflict resolution.", level="warning")
                                
                                # Try to extract and display the conflicting packages
                                conflict_lines = [line for line in process.stderr.splitlines() if "depends on" in line]
                                if conflict_lines:
                                    self.log("Conflict details:", level="warning")
                                    for line in conflict_lines:
                                        self.log(f"  {line}", level="warning")
                        
                        success = False
                finally:
                    # Clean up temporary file for this group
                    if temp_file and temp_file.exists():
                        temp_file.unlink()
            
            if success:
                if self.total_bytes_saved > 0:
                    self.log(f"Cache saved approximately {self._format_size(self.total_bytes_saved)} of downloads", 
                           level="success")
                self.log("Installation completed successfully", level="success")
        
        except Exception as e:
            self.log(f"Error during package installation: {e}", level="error")
            if self.verbosity >= self.VERBOSITY["debug"]:
                import traceback
                self.log(traceback.format_exc(), level="debug")
            success = False
        
        return success
    
    def _setup_pip_download_cache(self):
        """Set up HTTP cache handler for pip downloads."""
        import pip._vendor.requests.adapters
        
        # Save original method
        original_send = pip._vendor.requests.adapters.HTTPAdapter.send
        
        def cached_send(self, request, **kwargs):
            """Intercept HTTP requests to check cache before downloading."""
            if request.method == "GET" and request.url.startswith(("http://", "https://")):
                # Only cache packages, not index or metadata requests
                if any(ext in request.url for ext in ['.whl', '.tar.gz', '.zip']):
                    cached_response = self._get_from_cache(request.url)
                    if cached_response:
                        return cached_response
            
            # Proceed with original request if no cache hit
            return original_send(self, request, **kwargs)
        
        # Monkey patch the send method
        pip._vendor.requests.adapters.HTTPAdapter.send = cached_send
    
    def _get_from_cache(self, url: str):
        """Check if URL is in cache and return cached response if valid."""
        if url in self.cached_files:
            cached_file = self.cached_files[url]
            
            if cached_file.local_path.exists():
                self.log(f"Cache hit for {url}", level="debug")
                
                # Check if we need to validate cache with server
                needs_validation = False
                
                if needs_validation:
                    # Implement conditional GET logic here if needed
                    pass
                else:
                    # Use cached file without validation
                    self.total_bytes_saved += cached_file.size
                    return self._create_response_from_cache(cached_file)
        
        # Cache miss or invalid cache
        self.log(f"Cache miss for {url}", level="debug")
        return None
    
    def _create_response_from_cache(self, cached_file: CachedFile):
        """Create a requests.Response object from cached file."""
        # This is a simplified mock - in a real implementation, 
        # we would create a proper Response object
        class MockResponse:
            def __init__(self, path, url):
                self.status_code = 200
                self._content = open(path, 'rb').read()
                self.url = url
                self.headers = {'Content-Length': str(len(self._content))}
            
            @property
            def content(self):
                return self._content
        
        return MockResponse(cached_file.local_path, cached_file.url)
    
    def _format_size(self, size_bytes: int) -> str:
        """Format byte size to human readable format."""
        if size_bytes < 1024:
            return f"{size_bytes} bytes"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    def validate_dependencies(self, packages: Dict[str, Package]) -> bool:
        """Check for common dependency issues."""
        if not packages:
            return True
        
        issues_found = False
        
        # Check for packages that might conflict
        known_conflicts = {
            ('tensorflow', 'tensorflow-gpu'): "Use either tensorflow or tensorflow-gpu, not both",
            ('typing', 'typing-extensions'): "typing is included in Python 3.5+, prefer typing-extensions",
        }
        
        for conflict_pair, message in known_conflicts.items():
            if all(p.lower() in packages for p in conflict_pair):
                self.log(f"Potential conflict: {message}", level="warning")
                issues_found = True
        
        # Check for outdated packages or security issues
        if not self.args.dry_run and self.verbosity >= self.VERBOSITY["verbose"]:
            try:
                safety_result = subprocess.run(
                    [sys.executable, "-m", "pip", "list", "--outdated"],
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                if safety_result.returncode == 0 and safety_result.stdout:
                    outdated = [line for line in safety_result.stdout.splitlines() 
                               if any(p.lower() in line.lower() for p in packages)]
                    if outdated:
                        self.log("The following packages have newer versions available:", level="warning")
                        for line in outdated[2:]:  # Skip header lines
                            self.log(f"  {line}", level="warning")
                        issues_found = True
            except Exception as e:
                self.log(f"Error checking for outdated packages: {e}", level="error")
        
        return not issues_found
    
    def run(self):
        """Run the main workflow with dynamic conflict resolution."""
        # Find requirements files
        self.requirements_files = self.find_requirements_files()
        
        # If no requirements files found, exit
        if not self.requirements_files:
            self.log("No requirements.txt files found", level="error")
            return False
        
        # First, collect all packages and identify conflicts
        conflicts, all_packages = self._collect_packages_and_identify_conflicts()
        
        # Check if we should resolve conflicts interactively
        if conflicts and self.args.interactive and len(self.requirements_files) > 1:
            resolved_packages = self._resolve_conflicts_dynamically(conflicts, all_packages)
        else:
            # Use default resolution strategy
            resolved_packages = self._resolve_conflicts_automatically(all_packages)
        
        # Validate dependencies
        self.validate_dependencies(resolved_packages)
        
        # Write aggregated requirements
        self.write_aggregated_requirements(resolved_packages)
        
        # Install packages if not in no-install mode
        if not self.args.no_install:
            return self.install_packages(resolved_packages)
        
        return True

    def _collect_packages_and_identify_conflicts(self):
        """Collect all packages and identify conflicts."""
        all_packages = defaultdict(list)
        
        # Collect all package versions
        for req_file in self.requirements_files:
            packages = self.parse_requirements_file(req_file)
            for package in packages:
                if package.name.startswith(('http://', 'https://', 'git+')):
                    # For direct URLs, use URL as key
                    all_packages[package.name].append(package)
                else:
                    all_packages[package.name.lower()].append(package)
        
        # Identify conflicts
        conflicts = {}
        for name, package_variants in all_packages.items():
            if len(package_variants) > 1:
                # Check if there's an actual conflict in version constraints
                version_constraints = {p.version_constraint for p in package_variants}
                if len(version_constraints) > 1:
                    # There's a genuine conflict
                    conflicts[name] = package_variants
        
        return conflicts, all_packages

    def _get_extension_display_name(self, file_path):
        """Get a human-readable display name for an extension."""
        path = Path(file_path)
        
        if self.base_requirements and path == self.base_requirements:
            return "base requirements"
        
        if self.args.extensions:
            try:
                ext_dir = Path(self.args.extensions).expanduser()
                rel_path = path.relative_to(ext_dir)
                return f"{ext_dir.name}/{rel_path}"
            except ValueError:
                pass
        
        return str(path)

    def _resolve_conflicts_dynamically(self, conflicts, all_packages):
        """Resolve conflicts dynamically by asking about specific conflicting extensions."""
        resolved_packages = {}
        extension_priorities = {}  # Maps file paths to their priority (lower is higher priority)
        
        if not conflicts:
            self.log("No package conflicts detected.", level="info")
            return self._resolve_conflicts_automatically(all_packages)
        
        # Find which extensions are involved in conflicts
        conflicting_extensions = set()
        for _, variants in conflicts.items():
            for variant in variants:
                conflicting_extensions.add(variant.source_file)
        
        # Sort conflicting extensions for consistent display
        conflicting_extensions = sorted(conflicting_extensions)
        
        print(f"\n{COLORS['bold']}CONFLICT RESOLUTION{COLORS['reset']}")
        print(f"Found {len(conflicts)} package conflicts across {len(conflicting_extensions)} extensions.")
        
        # Show which extensions are involved in conflicts
        print("\nConflicting extensions:")
        for i, ext_path in enumerate(conflicting_extensions, 1):
            display_name = self._get_extension_display_name(ext_path)
            print(f"{i}. {display_name}")
        
        # Group conflicts by the extensions involved
        conflict_groups = self._group_conflicts_by_extensions(conflicts)
        
        # Process each conflict group
        for group_num, (ext_group, pkg_conflicts) in enumerate(conflict_groups.items(), 1):
            # Format the extension group for display
            ext_names = [self._get_extension_display_name(ext) for ext in ext_group]
            
            print(f"\n{COLORS['bold']}Conflict Group {group_num}: {', '.join(ext_names)}{COLORS['reset']}")
            print(f"These extensions have conflicts for {len(pkg_conflicts)} package(s):")
            
            # Show conflicting packages
            for pkg_name in pkg_conflicts:
                variants = conflicts[pkg_name]
                print(f"  - {pkg_name}: " + ", ".join(
                    f"{v.version_constraint} (from {self._get_extension_display_name(v.source_file)})" 
                    for v in variants if v.source_file in ext_group
                ))
            
            # Ask for priority
            print("\nWhich extension should take priority for these conflicts?")
            for i, ext in enumerate(ext_group, 1):
                print(f"{i}. {self._get_extension_display_name(ext)}")
            
            choice = None
            while choice is None:
                try:
                    user_input = input("Enter your choice (number): ")
                    idx = int(user_input.strip()) - 1
                    if 0 <= idx < len(ext_group):
                        choice = ext_group[idx]
                        # Assign priority (lower is higher priority)
                        for i, ext in enumerate(ext_group):
                            current_priority = extension_priorities.get(ext, float('inf'))
                            if ext == choice:
                                # Highest priority (lowest number)
                                extension_priorities[ext] = min(current_priority, 0)
                            else:
                                # Lower priority (higher number)
                                extension_priorities[ext] = min(current_priority, len(extension_priorities) + 1)
                    else:
                        print(f"Please enter a number between 1 and {len(ext_group)}")
                except ValueError:
                    print("Please enter a valid number")
        
        # Process all package variants with the established priorities
        for name, variants in all_packages.items():
            if len(variants) == 1:
                # No conflict, just use the single variant
                resolved_packages[name] = variants[0]
            elif name in conflicts:
                # Sort variants by extension priority
                prioritized = sorted(variants, key=lambda v: extension_priorities.get(v.source_file, float('inf')))
                resolved_packages[name] = prioritized[0]
            else:
                # Multiple variants but no real conflict (same version constraints)
                resolved_packages[name] = variants[0]
        
        # Summarize decisions
        print(f"\n{COLORS['green']}Conflict Resolution Summary:{COLORS['reset']}")
        for name, variants in conflicts.items():
            chosen = resolved_packages[name]
            print(f"  {name}: Using {chosen.qualified_name} from {self._get_extension_display_name(chosen.source_file)}")
        
        return resolved_packages

    def _group_conflicts_by_extensions(self, conflicts):
        """Group conflicts by the sets of extensions involved."""
        # First, build a mapping of which extensions conflict with each other
        extension_conflicts = defaultdict(set)
        
        for name, variants in conflicts.items():
            # Get all pairs of conflicting extensions
            ext_paths = [v.source_file for v in variants]
            for i, ext1 in enumerate(ext_paths):
                for ext2 in ext_paths[i+1:]:
                    extension_conflicts[ext1].add(ext2)
                    extension_conflicts[ext2].add(ext1)
        
        # Now group extensions that share conflicts
        visited = set()
        groups = {}
        
        for ext in extension_conflicts.keys():
            if ext in visited:
                continue
            
            # Find all connected extensions (those that share conflicts)
            group = set([ext])
            queue = [ext]
            
            while queue:
                current = queue.pop(0)
                for connected in extension_conflicts[current]:
                    if connected not in group:
                        group.add(connected)
                        queue.append(connected)
            
            # Check if this group has package conflicts
            group_conflicts = []
            for name, variants in conflicts.items():
                variant_sources = {v.source_file for v in variants}
                if any(source in group for source in variant_sources):
                    group_conflicts.append(name)
            
            if group_conflicts:
                # Convert group to sorted tuple to use as dictionary key
                group_tuple = tuple(sorted(group))
                groups[group_tuple] = group_conflicts
                
            visited.update(group)
        
        return groups

    def _resolve_conflicts_automatically(self, all_packages):
        """Resolve conflicts automatically using default strategies."""
        resolved_packages = {}
        
        for name, variants in all_packages.items():
            if len(variants) == 1:
                # No conflict
                resolved_packages[name] = variants[0]
            else:
                # Check for conflicts
                version_constraints = {p.version_constraint for p in variants}
                if len(version_constraints) == 1:
                    # Same version constraint across all occurrences
                    resolved_packages[name] = variants[0]
                else:
                    # Version conflict detected
                    if self.args.resolve_conflicts:
                        # Use standard resolution strategies
                        if self.args.upgrade:
                            # If no specific version in any requirement, choose least restrictive for upgrade
                            if all(not p.version_constraint or p.version_constraint.startswith('>') for p in variants):
                                resolved_packages[name] = self._get_least_restrictive(variants)
                            else:
                                # Otherwise, prioritize specific version constraints
                                resolved_packages[name] = self._get_most_restrictive(variants)
                        else:
                            # Choose most restrictive for stability
                            resolved_packages[name] = self._get_most_restrictive(variants)
                    else:
                        # Default to first occurrence if not resolving
                        resolved_packages[name] = variants[0]
                        self.log(f"Version conflict for {name}: {[p.qualified_name for p in variants]}", 
                               level="warning")
        
        return resolved_packages

def check_environment_safety():
    """
    Check if the script is running in a virtual environment, conda base,
    or system Python. Exits with an error if it's not in a safe environment.
    """
    import os
    import sys
    import platform
    
    # Initialize message variables
    env_type = "unknown"
    is_unsafe = False
    
    # Check if we're in a virtual environment of any kind
    in_virtualenv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
    # Check for Conda environment
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    
    if not in_virtualenv and not conda_env:
        # Not in any virtual environment
        env_type = "system Python"
        is_unsafe = True
    elif conda_env and conda_env.lower() in ('base', 'root'):
        # In conda base environment
        env_type = f"conda base environment ({conda_env})"
        is_unsafe = True
    
    if is_unsafe:
        print(f"\n{COLORS['red']}{COLORS['bold']}ERROR: UNSAFE ENVIRONMENT DETECTED{COLORS['reset']}")
        print(f"This script is running in the {env_type}, which is unsafe for package management.")
        print("\nRunning pip commands in this environment could break your system or conda installation.")
        
        print(f"\n{COLORS['yellow']}What to do instead:{COLORS['reset']}")
        if "conda" in env_type:
            print("""
1. Create a new conda environment:
   conda create -n your_env_name python=3.10

2. Activate the environment:
   conda activate your_env_name

3. Then run this script again in that environment.
""")
        else:
            print("""
1. Create a virtual environment:
   Option 1 (venv): python -m venv /path/to/new/virtual/environment
   Option 2 (conda): conda create -n your_env_name python=3.10

2. Activate the environment:
   Option 1 (venv): source /path/to/new/virtual/environment/bin/activate (Linux/macOS)
                   or \\path\\to\\new\\virtual\\environment\\Scripts\\activate (Windows)
   Option 2 (conda): conda activate your_env_name

3. Then run this script again in that environment.
""")
        
        print(f"{COLORS['yellow']}Why this matters:{COLORS['reset']}")
        print("""
Package management operations can have unintended side effects when run in system-wide
Python or conda base environments. These operations can break system tools or make your
conda installation unstable.

Virtual environments provide isolation that keeps package management operations safe.
""")
        sys.exit(1)

def main():
    """Parse arguments and run the manager."""
    # Safety check first, before anything else
    check_environment_safety()
    
    parser = argparse.ArgumentParser(
        description="Python Requirements Manager",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument("-r", "--requirements", 
                        help="Specify base project requirements file")
    parser.add_argument("-e", "--extensions", 
                        help="Directory to search for requirements.txt files")
    parser.add_argument("-a", "--aggregate", default="pipreqs.txt",
                        help="Output aggregated requirements file (default: pipreqs.txt)")
    parser.add_argument("-c", "--cache", 
                        help="Custom cache directory (default: ~/.cache/pipreqs)")
    parser.add_argument("-u", "--upgrade", action="store_true", 
                        help="Upgrade packages to latest versions")
    parser.add_argument("-v", "--verbose", action="count", default=0,
                        help="Enable verbose output (use -vv for even more detail)")
    parser.add_argument("-d", "--dry-run", action="store_true", 
                        help="Show what would be done without making changes")
    parser.add_argument("-i", "--interactive", action="store_true",
                        help="Enable interactive extension prioritization")
    parser.add_argument("--install", action="store_true", dest="do_install", 
                        help="Install packages after processing (default behavior)")
    parser.add_argument("--no-install", action="store_false", dest="do_install", 
                        help="Skip installation, only aggregate requirements")
    parser.add_argument("--no-resolve-conflicts", action="store_false", dest="resolve_conflicts", 
                        help="Do not attempt to resolve version conflicts")
    parser.add_argument("--ignore-environment-check", action="store_true",
                        help=argparse.SUPPRESS)  # Hidden option for testing/development
    
    parser.set_defaults(do_install=True, resolve_conflicts=True)
    
    args = parser.parse_args()
    
    # Skip environment check if explicitly requested (hidden option)
    if not args.ignore_environment_check:
        check_environment_safety()
    
    # Convert do_install to no_install for internal use
    args.no_install = not args.do_install
    
    manager = PipReqsManager(args)
    success = manager.run()
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())